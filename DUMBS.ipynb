{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFgFuwc6IzA4",
        "outputId": "a861dc52-a758-4aa8-a96c-482770e6c1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload all datasets to your google drive in advance!\n",
        "Since there are 4 datsets, it is too slow to upload them individually in the colab notebook. Please refer to this link : https://drive.google.com/drive/folders/1-Q0fdugxlWb1n5dkeiGfe_TR9czBEx2i?usp=drive_link\n"
      ],
      "metadata": {
        "id": "noJnYZ0jtWKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define path to your uploaded Datasets\n",
        "Give the path to the folder that contains all datasets.\n",
        "For example : \"/content/drive/MyDrive/DUMBS\"\n",
        "\n",
        "In \"/content/drive/MyDrive/DUMBS\", it should contains\n",
        "1. \"/content/drive/MyDrive/DUMBS/Amazon\"\n",
        "2. \"/content/drive/MyDrive/DUMBS/Planetoid\"\n",
        "3. \"/content/drive/MyDrive/DUMBS/elliptic\"\n",
        "4. \"/content/drive/MyDrive/DUMBS/twitch\"\n",
        "5. \"/content/drive/MyDrive/DUMBS/params.csv\""
      ],
      "metadata": {
        "id": "_6oXjq41tsS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATADIR = \"/content/drive/MyDrive/DUMBS\" # ie : \"/content/drive/MyDrive/DUMBS\"\n",
        "PARAM_CSV_PATH = \"/content/drive/MyDrive/params.csv\" # upload params.csv file to the drive in advance"
      ],
      "metadata": {
        "id": "AO7DtXgetw6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma0jNXMnHIQ8"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XB7MPt5CtVRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vJR4eBFWjxf",
        "outputId": "2cb5a17d-bb3d-4e90-fef6-58f86185ba72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu122.html\n",
            "Requirement already satisfied: pyg_lib in /usr/local/lib/python3.10/dist-packages (0.4.0+pt22cu121)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt22cu121)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt22cu121)\n",
            "Requirement already satisfied: torch_cluster in /usr/local/lib/python3.10/dist-packages (1.6.3+pt22cu121)\n",
            "Requirement already satisfied: torch_spline_conv in /usr/local/lib/python3.10/dist-packages (1.2.2+pt22cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.6.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.0+cu122.html\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gYlww5o2xr4"
      },
      "source": [
        "#Synthetic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5uzGMXIN6hx"
      },
      "source": [
        "##parse.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap_QoFSw3vum"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parser_add_main_args(parser):\n",
        "    # dataset and protocol\n",
        "    parser.add_argument('--data_dir', type=str, default='../data') # need to be specified\n",
        "    parser.add_argument('--dataset', type=str, default='cora')\n",
        "    parser.add_argument('--sub_dataset', type=str, default='')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--gnn_gen', type=str, default='gcn', choices=['gcn', 'gat', 'sgc'],\n",
        "                        help='random initialized gnn for data generation')\n",
        "    parser.add_argument('--rocauc', action='store_true',\n",
        "                        help='set the eval function to rocauc')\n",
        "\n",
        "    # model\n",
        "    parser.add_argument('--hidden_channels', type=int, default=32)\n",
        "    parser.add_argument('--dropout', type=float, default=0.0)\n",
        "    parser.add_argument('--gnn', type=str, default='gcn')\n",
        "    parser.add_argument('--method', type=str, default='erm',\n",
        "                        choices=['erm', 'eerm'], help='gnn backbone')\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers for deep methods')\n",
        "    parser.add_argument('--no_bn', action='store_true', help='do not use batchnorm')\n",
        "\n",
        "    # training\n",
        "    parser.add_argument('--lr', type=float, default=0.01)\n",
        "    parser.add_argument('--epochs', type=int, default=200)\n",
        "    parser.add_argument('--cpu', action='store_true')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-3)\n",
        "    parser.add_argument('--runs', type=int, default=5,\n",
        "                        help='number of distinct runs')\n",
        "    parser.add_argument('--cached', action='store_true',\n",
        "                        help='set to use faster sgc')\n",
        "    parser.add_argument('--gat_heads', type=int, default=4,\n",
        "                        help='attention heads for gat')\n",
        "    parser.add_argument('--lp_alpha', type=float, default=.1,\n",
        "                        help='alpha for label prop')\n",
        "    parser.add_argument('--gpr_alpha', type=float, default=.1,\n",
        "                        help='alpha for gprgnn')\n",
        "    parser.add_argument('--gcnii_alpha', type=float, default=.1,\n",
        "                        help='alpha for gcnii')\n",
        "    parser.add_argument('--gcnii_lamda', type=float, default=1.0,\n",
        "                        help='lambda for gcnii')\n",
        "    parser.add_argument('--directed', action='store_true',\n",
        "                        help='set to not symmetrize adjacency')\n",
        "    parser.add_argument('--display_step', type=int,\n",
        "                        default=100, help='how often to print')\n",
        "\n",
        "    # for graph edit model\n",
        "    parser.add_argument('--K', type=int, default=3,\n",
        "                        help='num of views for data augmentation')\n",
        "    parser.add_argument('--T', type=int, default=1,\n",
        "                        help='steps for graph learner before one step for GNN')\n",
        "    parser.add_argument('--num_sample', type=int, default=5,\n",
        "                        help='num of samples for each node with graph edit')\n",
        "    parser.add_argument('--beta', type=float, default=1.0,\n",
        "                        help='weight for mean of risks from multiple domains')\n",
        "    parser.add_argument('--lr_a', type=float, default=0.005,\n",
        "                        help='learning rate for graph edit model')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJldxM2YN97F"
      },
      "source": [
        "##dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuIsQLMu3p0y"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "from torch_geometric.datasets import Planetoid, Amazon\n",
        "\n",
        "# from data_utils import rand_train_test_idx, even_quantile_labels, to_sparse_tensor, dataset_drive_url\n",
        "\n",
        "from os import path\n",
        "\n",
        "import pickle as pkl\n",
        "\n",
        "class NCDataset(object):\n",
        "    def __init__(self, name):\n",
        "        \"\"\"\n",
        "        based off of ogb NodePropPredDataset\n",
        "        https://github.com/snap-stanford/ogb/blob/master/ogb/nodeproppred/dataset.py\n",
        "        Gives torch tensors instead of numpy arrays\n",
        "            - name (str): name of the dataset\n",
        "            - root (str): root directory to store the dataset folder\n",
        "            - meta_dict: dictionary that stores all the meta-information about data. Default is None,\n",
        "                    but when something is passed, it uses its information. Useful for debugging for external contributers.\n",
        "\n",
        "        Usage after construction:\n",
        "\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "        graph, label = dataset[0]\n",
        "\n",
        "        Where the graph is a dictionary of the following form:\n",
        "        dataset.graph = {'edge_index': edge_index,\n",
        "                         'edge_feat': None,\n",
        "                         'node_feat': node_feat,\n",
        "                         'num_nodes': num_nodes}\n",
        "        For additional documentation, see OGB Library-Agnostic Loader https://ogb.stanford.edu/docs/nodeprop/\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.name = name  # original name, e.g., ogbn-proteins\n",
        "        self.graph = {}\n",
        "        self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx == 0, 'This dataset has only one graph'\n",
        "        return self.graph, self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.__class__.__name__, len(self))\n",
        "\n",
        "def load_nc_dataset(data_dir, dataname, sub_dataname='', gen_model='gcn'):\n",
        "    \"\"\" Loader for NCDataset\n",
        "        Returns NCDataset\n",
        "    \"\"\"\n",
        "    if dataname in  ('cora', 'amazon-photo'):\n",
        "        dataset = load_synthetic_dataset(data_dir, dataname, sub_dataname, gen_model)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "    return dataset\n",
        "\n",
        "def load_synthetic_dataset(data_dir, name, lang, gen_model='gcn'):\n",
        "    dataset = NCDataset(lang)\n",
        "\n",
        "    assert lang in range(0, 10), 'Invalid dataset'\n",
        "\n",
        "    if name == 'cora':\n",
        "        node_feat, y = pkl.load(open('{}/Planetoid/cora/gen/{}-{}.pkl'.format(data_dir, lang, gen_model), 'rb'))\n",
        "        torch_dataset = Planetoid(root='{}/Planetoid'.format(data_dir), name='cora')\n",
        "    elif name == 'amazon-photo':\n",
        "        node_feat, y = pkl.load(open('{}/Amazon/Photo/gen/{}-{}.pkl'.format(data_dir, lang, gen_model), 'rb'))\n",
        "        torch_dataset = Amazon(root='{}/Amazon'.format(data_dir), name='Photo')\n",
        "    data = torch_dataset[0]\n",
        "\n",
        "    edge_index = data.edge_index\n",
        "    label = y\n",
        "    num_nodes = node_feat.size(0)\n",
        "\n",
        "    dataset.graph = {'edge_index': edge_index,\n",
        "                     'node_feat': node_feat,\n",
        "                     'edge_feat': None,\n",
        "                     'num_nodes': num_nodes}\n",
        "\n",
        "    dataset.label = label\n",
        "\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMHeFfALOIVe"
      },
      "source": [
        "##main_as_utils.py (cora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0b8wqpiW2zFq",
        "outputId": "87bb3ffb-e4d1-4e3b-92cf-3a15892cb8bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "/usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-003bbe6ce866>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_undirected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_scatter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_scatter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda_spec\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcpu_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BUILD_DOCS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         raise ImportError(f\"Could not find module '{library}_cpu' in \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_scatter import scatter\n",
        "\n",
        "# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "fix_seed(0)\n",
        "\n",
        "def parse_args(parser, args=None, namespace=None):\n",
        "    args, argv = parser.parse_known_args(args, namespace)\n",
        "    return args\n",
        "\n",
        "parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
        "parser_add_main_args(parser)\n",
        "args = parse_args(parser)\n",
        "print(args)\n",
        "\n",
        "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "args.dataset = 'cora'\n",
        "def get_dataset(dataset, sub_dataset=None, gen_model=None):\n",
        "    ### Load and preprocess data ###\n",
        "    # args.data_dir = \"GraphOOD_EERM/data\"\n",
        "    args.data_dir = DATADIR\n",
        "    if dataset == 'cora':\n",
        "        dataset = load_nc_dataset(args.data_dir, 'cora', sub_dataset, gen_model)\n",
        "    elif dataset == 'amazon-photo':\n",
        "        dataset = load_nc_dataset(args.data_dir, 'amazon-photo', sub_dataset, gen_model)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "\n",
        "    if len(dataset.label.shape) == 1:\n",
        "        dataset.label = dataset.label.unsqueeze(1)\n",
        "\n",
        "    dataset.n = dataset.graph['num_nodes']\n",
        "    dataset.c = max(dataset.label.max().item() + 1, dataset.label.shape[1])\n",
        "    dataset.d = dataset.graph['node_feat'].shape[1]\n",
        "\n",
        "    dataset.graph['edge_index'], dataset.graph['node_feat'] = \\\n",
        "        dataset.graph['edge_index'], dataset.graph['node_feat']\n",
        "    return dataset\n",
        "\n",
        "if args.dataset == 'cora':\n",
        "    tr_sub, val_sub, te_subs = [0], [1], list(range(2, 10))\n",
        "    gen_model = args.gnn_gen\n",
        "    dataset_tr = get_dataset(dataset='cora', sub_dataset=tr_sub[0], gen_model=gen_model)\n",
        "    dataset_val = get_dataset(dataset='cora', sub_dataset=val_sub[0], gen_model=gen_model)\n",
        "    datasets_te = [get_dataset(dataset='cora', sub_dataset=te_subs[i], gen_model=gen_model) for i in range(len(te_subs))]\n",
        "elif args.dataset == 'amazon-photo':\n",
        "    tr_sub, val_sub, te_subs = [0], [1], list(range(2, 10))\n",
        "    gen_model = args.gnn_gen\n",
        "    dataset_tr = get_dataset(dataset='amazon-photo', sub_dataset=tr_sub[0], gen_model=gen_model)\n",
        "    dataset_val = get_dataset(dataset='amazon-photo', sub_dataset=val_sub[0], gen_model=gen_model)\n",
        "    datasets_te = [get_dataset(dataset='amazon-photo', sub_dataset=te_subs[i], gen_model=gen_model) for i in range(len(te_subs))]\n",
        "else:\n",
        "    raise ValueError('Invalid dataname')\n",
        "\n",
        "print(f\"Train num nodes {dataset_tr.n} | num classes {dataset_tr.c} | num node feats {dataset_tr.d}\")\n",
        "print(f\"Val num nodes {dataset_val.n} | num classes {dataset_val.c} | num node feats {dataset_val.d}\")\n",
        "for i in range(len(te_subs)):\n",
        "    dataset_te = datasets_te[i]\n",
        "    print(f\"Test {i} num nodes {dataset_te.n} | num classes {dataset_te.c} | num node feats {dataset_te.d}\")\n",
        "\n",
        "dataset_tr_cora = dataset_tr\n",
        "dataset_val_cora = dataset_val\n",
        "datasets_te_cora = datasets_te\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoqsZ5h6OOZa"
      },
      "source": [
        "##main_as_utils_photo.py (amazon-photo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vW1NrlwONzn"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_scatter import scatter\n",
        "\n",
        "# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\n",
        "def fix_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "fix_seed(0)\n",
        "\n",
        "def parse_args(parser, args=None, namespace=None):\n",
        "    args, argv = parser.parse_known_args(args, namespace)\n",
        "    return args\n",
        "\n",
        "parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
        "parser_add_main_args(parser)\n",
        "args = parse_args(parser)\n",
        "print(args)\n",
        "\n",
        "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "args.dataset = 'amazon-photo'\n",
        "def get_dataset(dataset, sub_dataset=None, gen_model=None):\n",
        "    ### Load and preprocess data ###\n",
        "    # args.data_dir = \"GraphOOD_EERM/data\"\n",
        "    args.data_dir = DATADIR\n",
        "    if dataset == 'cora':\n",
        "        dataset = load_nc_dataset(args.data_dir, 'cora', sub_dataset, gen_model)\n",
        "    elif dataset == 'amazon-photo':\n",
        "        dataset = load_nc_dataset(args.data_dir, 'amazon-photo', sub_dataset, gen_model)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "\n",
        "    if len(dataset.label.shape) == 1:\n",
        "        dataset.label = dataset.label.unsqueeze(1)\n",
        "\n",
        "    dataset.n = dataset.graph['num_nodes']\n",
        "    dataset.c = max(dataset.label.max().item() + 1, dataset.label.shape[1])\n",
        "    dataset.d = dataset.graph['node_feat'].shape[1]\n",
        "\n",
        "    dataset.graph['edge_index'], dataset.graph['node_feat'] = \\\n",
        "        dataset.graph['edge_index'], dataset.graph['node_feat']\n",
        "    return dataset\n",
        "\n",
        "if args.dataset == 'cora':\n",
        "    tr_sub, val_sub, te_subs = [0], [1], list(range(2, 10))\n",
        "    gen_model = args.gnn_gen\n",
        "    dataset_tr = get_dataset(dataset='cora', sub_dataset=tr_sub[0], gen_model=gen_model)\n",
        "    dataset_val = get_dataset(dataset='cora', sub_dataset=val_sub[0], gen_model=gen_model)\n",
        "    datasets_te = [get_dataset(dataset='cora', sub_dataset=te_subs[i], gen_model=gen_model) for i in range(len(te_subs))]\n",
        "elif args.dataset == 'amazon-photo':\n",
        "    tr_sub, val_sub, te_subs = [0], [1], list(range(2, 10))\n",
        "    gen_model = args.gnn_gen\n",
        "    dataset_tr = get_dataset(dataset='amazon-photo', sub_dataset=tr_sub[0], gen_model=gen_model)\n",
        "    dataset_val = get_dataset(dataset='amazon-photo', sub_dataset=val_sub[0], gen_model=gen_model)\n",
        "    datasets_te = [get_dataset(dataset='amazon-photo', sub_dataset=te_subs[i], gen_model=gen_model) for i in range(len(te_subs))]\n",
        "else:\n",
        "    raise ValueError('Invalid dataname')\n",
        "\n",
        "print(f\"Train num nodes {dataset_tr.n} | num classes {dataset_tr.c} | num node feats {dataset_tr.d}\")\n",
        "print(f\"Val num nodes {dataset_val.n} | num classes {dataset_val.c} | num node feats {dataset_val.d}\")\n",
        "for i in range(len(te_subs)):\n",
        "    dataset_te = datasets_te[i]\n",
        "    print(f\"Test {i} num nodes {dataset_te.n} | num classes {dataset_te.c} | num node feats {dataset_te.d}\")\n",
        "\n",
        "dataset_tr_amazon_photo = dataset_tr\n",
        "dataset_val_amazon_photo = dataset_val\n",
        "datasets_te_amazon_photo = datasets_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDGujqCGNflL"
      },
      "source": [
        "#multi-graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYykYQUqPdvi"
      },
      "source": [
        "##parse.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW9qQm2aNsD2"
      },
      "outputs": [],
      "source": [
        "def parser_add_main_args(parser):\n",
        "    # dataset and protocol\n",
        "    parser.add_argument('--data_dir', type=str, default='../data') # need to be specified\n",
        "    parser.add_argument('--dataset', type=str, default='twitch-e')\n",
        "    parser.add_argument('--sub_dataset', type=str, default='')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--rocauc', action='store_true',\n",
        "                        help='set the eval function to rocauc')\n",
        "\n",
        "    # model\n",
        "    parser.add_argument('--hidden_channels', type=int, default=32)\n",
        "    parser.add_argument('--dropout', type=float, default=0.0)\n",
        "    parser.add_argument('--gnn', type=str, default='gcn')\n",
        "    parser.add_argument('--method', type=str, default='erm',\n",
        "                        choices=['erm', 'eerm'])\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers for deep methods')\n",
        "    parser.add_argument('--no_bn', action='store_true', help='do not use batchnorm')\n",
        "\n",
        "    # training\n",
        "    parser.add_argument('--lr', type=float, default=0.01)\n",
        "    parser.add_argument('--epochs', type=int, default=200)\n",
        "    parser.add_argument('--cpu', action='store_true')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-3)\n",
        "    parser.add_argument('--display_step', type=int,\n",
        "                        default=1, help='how often to print')\n",
        "    parser.add_argument('--runs', type=int, default=5,\n",
        "                        help='number of distinct runs')\n",
        "    parser.add_argument('--cached', action='store_true',\n",
        "                        help='set to use faster sgc')\n",
        "    parser.add_argument('--gat_heads', type=int, default=4,\n",
        "                        help='attention heads for gat')\n",
        "    parser.add_argument('--lp_alpha', type=float, default=.1,\n",
        "                        help='alpha for label prop')\n",
        "    parser.add_argument('--gpr_alpha', type=float, default=.1,\n",
        "                        help='alpha for gprgnn')\n",
        "    parser.add_argument('--gcnii_alpha', type=float, default=.1,\n",
        "                        help='alpha for gcnii')\n",
        "    parser.add_argument('--gcnii_lamda', type=float, default=1.0,\n",
        "                        help='lambda for gcnii')\n",
        "    parser.add_argument('--directed', action='store_true',\n",
        "                        help='set to not symmetrize adjacency')\n",
        "\n",
        "    # for graph edit model\n",
        "    parser.add_argument('--K', type=int, default=3,\n",
        "                        help='num of views for data augmentation')\n",
        "    parser.add_argument('--T', type=int, default=1,\n",
        "                        help='steps for graph learner before one step for GNN')\n",
        "    parser.add_argument('--num_sample', type=int, default=5,\n",
        "                        help='num of samples for each node with graph edit')\n",
        "    parser.add_argument('--beta', type=float, default=1.0,\n",
        "                        help='weight for mean of risks from multiple domains')\n",
        "    parser.add_argument('--lr_a', type=float, default=0.005,\n",
        "                        help='learning rate for graph learner with graph edit')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy5P7EycP3y5"
      },
      "source": [
        "##load_data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y72YWRcAP5yV"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "import torch\n",
        "import csv\n",
        "import json\n",
        "from os import path\n",
        "import pickle as pkl\n",
        "\n",
        "def load_fb100(data_dir, filename):\n",
        "    mat = scipy.io.loadmat(f'{data_dir}/facebook100/{filename}.mat')\n",
        "    A = mat['A']\n",
        "    metadata = mat['local_info']\n",
        "    return A, metadata\n",
        "\n",
        "def load_twitch(data_dir, lang):\n",
        "    assert lang in ('DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU', 'TW'), 'Invalid dataset'\n",
        "    filepath = f\"{data_dir}/twitch/{lang}\"\n",
        "    label = []\n",
        "    node_ids = []\n",
        "    src = []\n",
        "    targ = []\n",
        "    uniq_ids = set()\n",
        "    with open(f\"{filepath}/musae_{lang}_target.csv\", 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            node_id = int(row[5])\n",
        "            # handle FR case of non-unique rows\n",
        "            if node_id not in uniq_ids:\n",
        "                uniq_ids.add(node_id)\n",
        "                label.append(int(row[2]==\"True\"))\n",
        "                node_ids.append(int(row[5]))\n",
        "\n",
        "    node_ids = np.array(node_ids, dtype=int)\n",
        "    with open(f\"{filepath}/musae_{lang}_edges.csv\", 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            src.append(int(row[0]))\n",
        "            targ.append(int(row[1]))\n",
        "    with open(f\"{filepath}/musae_{lang}_features.json\", 'r') as f:\n",
        "        j = json.load(f)\n",
        "    src = np.array(src)\n",
        "    targ = np.array(targ)\n",
        "    label = np.array(label)\n",
        "    inv_node_ids = {node_id:idx for (idx, node_id) in enumerate(node_ids)}\n",
        "    reorder_node_ids = np.zeros_like(node_ids)\n",
        "    for i in range(label.shape[0]):\n",
        "        reorder_node_ids[i] = inv_node_ids[i]\n",
        "\n",
        "    n = label.shape[0]\n",
        "    A = scipy.sparse.csr_matrix((np.ones(len(src)),\n",
        "                                 (np.array(src), np.array(targ))),\n",
        "                                shape=(n,n))\n",
        "    features = np.zeros((n,3170))\n",
        "    for node, feats in j.items():\n",
        "        if int(node) >= n:\n",
        "            continue\n",
        "        features[int(node), np.array(feats, dtype=int)] = 1\n",
        "    # features = features[:, np.sum(features, axis=0) != 0] # remove zero cols. not need for cross graph task\n",
        "    new_label = label[reorder_node_ids]\n",
        "    label = new_label\n",
        "\n",
        "    return A, label, features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNTF2mSyPuFY"
      },
      "source": [
        "##dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvN4sNFUPrCA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "\n",
        "from os import path\n",
        "\n",
        "import pickle as pkl\n",
        "\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "class NCDataset(object):\n",
        "    def __init__(self, name):\n",
        "        \"\"\"\n",
        "        based off of ogb NodePropPredDataset\n",
        "        https://github.com/snap-stanford/ogb/blob/master/ogb/nodeproppred/dataset.py\n",
        "        Gives torch tensors instead of numpy arrays\n",
        "            - name (str): name of the dataset\n",
        "            - root (str): root directory to store the dataset folder\n",
        "            - meta_dict: dictionary that stores all the meta-information about data. Default is None,\n",
        "                    but when something is passed, it uses its information. Useful for debugging for external contributers.\n",
        "\n",
        "        Usage after construction:\n",
        "\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "        graph, label = dataset[0]\n",
        "\n",
        "        Where the graph is a dictionary of the following form:\n",
        "        dataset.graph = {'edge_index': edge_index,\n",
        "                         'edge_feat': None,\n",
        "                         'node_feat': node_feat,\n",
        "                         'num_nodes': num_nodes}\n",
        "        For additional documentation, see OGB Library-Agnostic Loader https://ogb.stanford.edu/docs/nodeprop/\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.name = name\n",
        "        self.graph = {}\n",
        "        self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx == 0, 'This dataset has only one graph'\n",
        "        return self.graph, self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.__class__.__name__, len(self))\n",
        "\n",
        "def load_nc_dataset(data_dir, dataname, sub_dataname=''):\n",
        "    \"\"\" Loader for NCDataset\n",
        "        Returns NCDataset\n",
        "    \"\"\"\n",
        "    if dataname == 'twitch-e':\n",
        "        # twitch-explicit graph\n",
        "        if sub_dataname not in ('DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU', 'TW'):\n",
        "            print('Invalid sub_dataname, deferring to DE graph')\n",
        "            sub_dataname = 'DE'\n",
        "        dataset = load_twitch_dataset(data_dir, sub_dataname)\n",
        "    elif dataname == 'fb100':\n",
        "        if sub_dataname not in ('Penn94', 'Amherst41', 'Cornell5', 'Johns Hopkins55', 'Reed98', 'Caltech36', 'Berkeley13', 'Brown11', 'Columbia2', 'Yale4', 'Virginia63', 'Texas80',\n",
        "                                'Bingham82', 'Duke14', 'Princeton12', 'WashU32', 'Brandeis99', 'Carnegie49'):\n",
        "            print('Invalid sub_dataname, deferring to Penn94 graph')\n",
        "            sub_dataname = 'Penn94'\n",
        "        dataset = load_fb100_dataset(data_dir, sub_dataname)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "    return dataset\n",
        "\n",
        "def load_twitch_dataset(data_dir, lang):\n",
        "    assert lang in ('DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU', 'TW'), 'Invalid dataset'\n",
        "    A, label, features = load_twitch(data_dir, lang)\n",
        "    dataset = NCDataset(lang)\n",
        "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
        "    node_feat = torch.tensor(features, dtype=torch.float)\n",
        "    num_nodes = node_feat.shape[0]\n",
        "    dataset.graph = {'edge_index': edge_index,\n",
        "                     'edge_feat': None,\n",
        "                     'node_feat': node_feat,\n",
        "                     'num_nodes': num_nodes}\n",
        "    dataset.label = torch.tensor(label)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_fb100_dataset(data_dir, filename):\n",
        "    feature_vals_all = np.empty((0, 6))\n",
        "    for f in ['Penn94', 'Amherst41', 'Cornell5', 'Johns Hopkins55', 'Reed98', 'Caltech36', 'Berkeley13', 'Brown11', 'Columbia2', 'Yale4', 'Virginia63', 'Texas80',\n",
        "              'Bingham82', 'Duke14', 'Princeton12', 'WashU32', 'Brandeis99', 'Carnegie49']:\n",
        "        try:\n",
        "            A, metadata = load_fb100(data_dir, f)\n",
        "        except: # TODO\n",
        "            print(f'Warning: file not exist!!! {data_dir}, {f}')\n",
        "            continue\n",
        "        metadata = metadata.astype(int)\n",
        "        feature_vals = np.hstack(\n",
        "            (np.expand_dims(metadata[:, 0], 1), metadata[:, 2:]))\n",
        "        feature_vals_all = np.vstack(\n",
        "            (feature_vals_all, feature_vals)\n",
        "        )\n",
        "\n",
        "    A, metadata = load_fb100(data_dir, filename)\n",
        "    dataset = NCDataset(filename)\n",
        "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
        "    metadata = metadata.astype(int)\n",
        "    label = metadata[:, 1] - 1  # gender label, -1 means unlabeled\n",
        "\n",
        "    # make features into one-hot encodings\n",
        "    feature_vals = np.hstack(\n",
        "        (np.expand_dims(metadata[:, 0], 1), metadata[:, 2:]))\n",
        "    features = np.empty((A.shape[0], 0))\n",
        "    for col in range(feature_vals.shape[1]):\n",
        "        feat_col = feature_vals[:, col]\n",
        "        # feat_onehot = label_binarize(feat_col, classes=np.unique(feat_col))\n",
        "        feat_onehot = label_binarize(feat_col, classes=np.unique(feature_vals_all[:, col]))\n",
        "        features = np.hstack((features, feat_onehot))\n",
        "\n",
        "    node_feat = torch.tensor(features, dtype=torch.float)\n",
        "    num_nodes = metadata.shape[0]\n",
        "    dataset.graph = {'edge_index': edge_index,\n",
        "                     'edge_feat': None,\n",
        "                     'node_feat': node_feat,\n",
        "                     'num_nodes': num_nodes}\n",
        "    dataset.label = torch.tensor(label)\n",
        "    dataset.label = torch.where(dataset.label > 0, 1, 0)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLeFglToP-jO"
      },
      "source": [
        "##main_as_utils (twitch-e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSmPCzBLQB8R"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_scatter import scatter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\n",
        "def fix_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "fix_seed(0)\n",
        "\n",
        "def parse_args(parser, args=None, namespace=None):\n",
        "    args, argv = parser.parse_known_args(args, namespace)\n",
        "    return args\n",
        "\n",
        "parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
        "parser_add_main_args(parser)\n",
        "# args = parser.parse_args()\n",
        "args = parse_args(parser)\n",
        "print(args)\n",
        "\n",
        "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "args.dataset = 'twitch-e'\n",
        "def get_dataset(dataset, sub_dataset=None):\n",
        "    ### Load and preprocess data ###\n",
        "    if dataset == 'twitch-e':\n",
        "        # args.data_dir = \"GraphOOD_EERM/data\"\n",
        "        args.data_dir = DATADIR\n",
        "        dataset = load_nc_dataset(args.data_dir, 'twitch-e', sub_dataset)\n",
        "    elif dataset == 'fb100':\n",
        "        dataset = load_nc_dataset(args.data_dir, 'fb100', sub_dataset)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "\n",
        "    if len(dataset.label.shape) == 1:\n",
        "        dataset.label = dataset.label.unsqueeze(1)\n",
        "\n",
        "    dataset.n = dataset.graph['num_nodes']\n",
        "    dataset.c = max(dataset.label.max().item() + 1, dataset.label.shape[1])\n",
        "    dataset.d = dataset.graph['node_feat'].shape[1]\n",
        "\n",
        "    dataset.graph['edge_index'], dataset.graph['node_feat'] = \\\n",
        "        dataset.graph['edge_index'], dataset.graph['node_feat']\n",
        "    return dataset\n",
        "\n",
        "if args.dataset == 'twitch-e':\n",
        "    twitch_sub_name = ['DE', 'ENGB', 'ES', 'FR', 'PTBR', 'RU', 'TW']\n",
        "    tr_sub, val_sub, te_subs = ['DE'], ['ENGB'], ['ES', 'FR', 'PTBR', 'RU', 'TW']\n",
        "    dataset_tr = get_dataset(dataset='twitch-e', sub_dataset=tr_sub[0])\n",
        "    dataset_val = get_dataset(dataset='twitch-e', sub_dataset=val_sub[0])\n",
        "    datasets_te = [get_dataset(dataset='twitch-e', sub_dataset=te_subs[i]) for i in range(len(te_subs))]\n",
        "elif args.dataset == 'fb100':\n",
        "    '''\n",
        "    Configure different training sub-graphs\n",
        "    '''\n",
        "    tr_subs, val_subs, te_subs = ['Johns Hopkins55', 'Caltech36', 'Amherst41'], ['Cornell5', 'Yale4'],  ['Penn94', 'Brown11', 'Texas80']\n",
        "    # tr_subs, val_subs, te_subs = ['Bingham82', 'Duke14', 'Princeton12'], ['Cornell5', 'Yale4'],  ['Penn94', 'Brown11', 'Texas80']\n",
        "    # tr_subs, val_subs, te_subs = ['WashU32', 'Brandeis99', 'Carnegie49'], ['Cornell5', 'Yale4'], ['Penn94', 'Brown11', 'Texas80']\n",
        "    datasets_tr = [get_dataset(dataset='fb100', sub_dataset=tr_subs[i]) for i in range(len(tr_subs))]\n",
        "    datasets_val = [get_dataset(dataset='fb100', sub_dataset=val_subs[i]) for i in range(len(val_subs))]\n",
        "    datasets_te = [get_dataset(dataset='fb100', sub_dataset=te_subs[i]) for i in range(len(te_subs))]\n",
        "else:\n",
        "    raise ValueError('Invalid dataname')\n",
        "\n",
        "if args.dataset == 'fb100':\n",
        "    dataset_tr = datasets_tr[0]\n",
        "    dataset_val = datasets_val[0]\n",
        "print(f\"Train num nodes {dataset_tr.n} | num classes {dataset_tr.c} | num node feats {dataset_tr.d}\")\n",
        "print(f\"Val num nodes {dataset_val.n} | num classes {dataset_val.c} | num node feats {dataset_val.d}\")\n",
        "for i in range(len(te_subs)):\n",
        "    dataset_te = datasets_te[i]\n",
        "    print(f\"Test {i} num nodes {dataset_te.n} | num classes {dataset_te.c} | num node feats {dataset_te.d}\")\n",
        "\n",
        "dataset_tr_twitch_e = dataset_tr\n",
        "dataset_val_twitch_e = dataset_val\n",
        "datasets_te_twitch_e = datasets_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_qHjQ9GS9AV"
      },
      "source": [
        "#temp_elliptic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbKN74d_TC1q"
      },
      "source": [
        "##parse.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqCQwr8CTCKz"
      },
      "outputs": [],
      "source": [
        "def parser_add_main_args(parser):\n",
        "    # dataset and protocol\n",
        "    parser.add_argument('--data_dir', type=str, default='../../data') # need to be specified\n",
        "    parser.add_argument('--dataset', type=str, default='elliptic')\n",
        "    parser.add_argument('--sub_dataset', type=str, default='')\n",
        "    parser.add_argument('--device', type=int, default=0,\n",
        "                        help='which gpu to use if any (default: 0)')\n",
        "    parser.add_argument('--rocauc', action='store_true',\n",
        "                        help='set the eval function to rocauc')\n",
        "\n",
        "    # model\n",
        "    parser.add_argument('--hidden_channels', type=int, default=32)\n",
        "    parser.add_argument('--dropout', type=float, default=0.)\n",
        "    parser.add_argument('--gnn', type=str, default='gcn')\n",
        "    parser.add_argument('--method', type=str, default='erm',\n",
        "                        choices=['erm', 'eerm'])\n",
        "    parser.add_argument('--num_layers', type=int, default=2,\n",
        "                        help='number of layers for deep methods')\n",
        "    parser.add_argument('--no_bn', action='store_true', help='do not use batchnorm')\n",
        "\n",
        "    # training\n",
        "    parser.add_argument('--lr', type=float, default=0.01)\n",
        "    parser.add_argument('--epochs', type=int, default=200)\n",
        "    parser.add_argument('--cpu', action='store_true')\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-3)\n",
        "    parser.add_argument('--display_step', type=int,\n",
        "                        default=1, help='how often to print')\n",
        "    parser.add_argument('--runs', type=int, default=5,\n",
        "                        help='number of distinct runs')\n",
        "    parser.add_argument('--cached', action='store_true',\n",
        "                        help='set to use faster sgc')\n",
        "    parser.add_argument('--gat_heads', type=int, default=2,\n",
        "                        help='attention heads for gat')\n",
        "    parser.add_argument('--lp_alpha', type=float, default=.1,\n",
        "                        help='alpha for label prop')\n",
        "    parser.add_argument('--gpr_alpha', type=float, default=.1,\n",
        "                        help='alpha for gprgnn')\n",
        "    parser.add_argument('--directed', action='store_true',\n",
        "                        help='set to not symmetrize adjacency')\n",
        "\n",
        "    # for graph edit model\n",
        "    parser.add_argument('--K', type=int, default=3,\n",
        "                        help='num of views for data augmentation')\n",
        "    parser.add_argument('--T', type=int, default=1,\n",
        "                        help='steps for graph learner before one step for GNN')\n",
        "    parser.add_argument('--num_sample', type=int, default=5,\n",
        "                        help='num of samples for each node with graph edit')\n",
        "    parser.add_argument('--beta', type=float, default=1.0,\n",
        "                        help='weight for mean of risks from multiple domains')\n",
        "    parser.add_argument('--lr_a', type=float, default=0.005,\n",
        "                        help='learning rate for graph learner with graph edit')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm-LsexlTMZj"
      },
      "source": [
        "##dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSkKv-4ITKA_"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "\n",
        "from os import path\n",
        "\n",
        "import pickle as pkl\n",
        "\n",
        "class NCDataset(object):\n",
        "    def __init__(self, name):\n",
        "        \"\"\"\n",
        "        based off of ogb NodePropPredDataset\n",
        "        https://github.com/snap-stanford/ogb/blob/master/ogb/nodeproppred/dataset.py\n",
        "        Gives torch tensors instead of numpy arrays\n",
        "            - name (str): name of the dataset\n",
        "            - root (str): root directory to store the dataset folder\n",
        "            - meta_dict: dictionary that stores all the meta-information about data. Default is None,\n",
        "                    but when something is passed, it uses its information. Useful for debugging for external contributers.\n",
        "\n",
        "        Usage after construction:\n",
        "\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "        graph, label = dataset[0]\n",
        "\n",
        "        Where the graph is a dictionary of the following form:\n",
        "        dataset.graph = {'edge_index': edge_index,\n",
        "                         'edge_feat': None,\n",
        "                         'node_feat': node_feat,\n",
        "                         'num_nodes': num_nodes}\n",
        "        For additional documentation, see OGB Library-Agnostic Loader https://ogb.stanford.edu/docs/nodeprop/\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.name = name  # original name, e.g., ogbn-proteins\n",
        "        self.graph = {}\n",
        "        self.label = None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx == 0, 'This dataset has only one graph'\n",
        "        return self.graph, self.label\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.__class__.__name__, len(self))\n",
        "\n",
        "def load_nc_dataset(data_dir, dataname, sub_dataname=''):\n",
        "    \"\"\" Loader for NCDataset\n",
        "        Returns NCDataset\n",
        "    \"\"\"\n",
        "    if dataname == 'elliptic':\n",
        "        if sub_dataname not in range(0, 49):\n",
        "            print('Invalid sub_dataname, deferring to graph1')\n",
        "            sub_dataname = 0\n",
        "        dataset = load_elliptic_dataset(data_dir, sub_dataname)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "    return dataset\n",
        "\n",
        "def load_elliptic_dataset(data_dir, lang):\n",
        "    assert lang in range(0, 49), 'Invalid dataset'\n",
        "    result = pkl.load(open('{}/elliptic/{}.pkl'.format(data_dir, lang), 'rb'))\n",
        "    A, label, features = result\n",
        "    dataset = NCDataset(lang)\n",
        "    edge_index = torch.tensor(A.nonzero(), dtype=torch.long)\n",
        "    node_feat = torch.tensor(features, dtype=torch.float)\n",
        "    num_nodes = node_feat.shape[0]\n",
        "    dataset.graph = {'edge_index': edge_index,\n",
        "                     'edge_feat': None,\n",
        "                     'node_feat': node_feat,\n",
        "                     'num_nodes': num_nodes}\n",
        "    dataset.label = torch.tensor(label)\n",
        "    dataset.mask = (dataset.label >= 0)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYkdDZwHTRtp"
      },
      "source": [
        "##main_as_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKwMY7kyTT98"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_scatter import scatter\n",
        "\n",
        "# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\n",
        "def fix_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "fix_seed(0)\n",
        "\n",
        "def parse_args(parser, args=None, namespace=None):\n",
        "    args, argv = parser.parse_known_args(args, namespace)\n",
        "    return args\n",
        "\n",
        "parser = argparse.ArgumentParser(description='General Training Pipeline')\n",
        "parser_add_main_args(parser)\n",
        "# args = parser.parse_args()\n",
        "args = parse_args(parser)\n",
        "args.dataset = 'elliptic'\n",
        "print(args)\n",
        "\n",
        "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "def get_dataset(dataset, sub_dataset=None):\n",
        "    ### Load and preprocess data ###\n",
        "    if dataset == 'elliptic':\n",
        "        # args.data_dir = \"GraphOOD_EERM/data\"\n",
        "        args.data_dir = DATADIR\n",
        "        dataset = load_nc_dataset(args.data_dir, 'elliptic', sub_dataset)\n",
        "    else:\n",
        "        raise ValueError('Invalid dataname')\n",
        "\n",
        "    if len(dataset.label.shape) == 1:\n",
        "        dataset.label = dataset.label.unsqueeze(1)\n",
        "\n",
        "    dataset.n = dataset.graph['num_nodes']\n",
        "    dataset.c = max(dataset.label.max().item() + 1, dataset.label.shape[1])\n",
        "    dataset.d = dataset.graph['node_feat'].shape[1]\n",
        "\n",
        "    dataset.graph['edge_index'], dataset.graph['node_feat'] = \\\n",
        "        dataset.graph['edge_index'], dataset.graph['node_feat']\n",
        "\n",
        "    return dataset\n",
        "\n",
        "if args.dataset == 'elliptic':\n",
        "    tr_subs, val_subs, te_subs = [i for i in range(6, 11)], [i for i in range(11, 16)], [i for i in range(16, 49)]\n",
        "    # te_subs = [41]\n",
        "    datasets_tr = [get_dataset(dataset='elliptic', sub_dataset=tr_subs[i]) for i in range(len(tr_subs))]\n",
        "    datasets_val = [get_dataset(dataset='elliptic', sub_dataset=val_subs[i]) for i in range(len(val_subs))]\n",
        "    datasets_te = [get_dataset(dataset='elliptic', sub_dataset=te_subs[i]) for i in range(len(te_subs))]\n",
        "else:\n",
        "    raise ValueError('Invalid dataname')\n",
        "\n",
        "dataset_tr = datasets_tr[0]\n",
        "dataset_val = datasets_val[0]\n",
        "print(f\"Train num nodes {dataset_tr.n} | num classes {dataset_tr.c} | num node feats {dataset_tr.d}\")\n",
        "print(f\"Val num nodes {dataset_val.n} | num classes {dataset_val.c} | num node feats {dataset_val.d}\")\n",
        "for i in range(len(te_subs)):\n",
        "    dataset_te = datasets_te[i]\n",
        "    print(f\"Test {i} num nodes {dataset_te.n} | num classes {dataset_te.c} | num node feats {dataset_te.d}\")\n",
        "\n",
        "dataset_tr_elliptic = dataset_tr\n",
        "dataset_val_elliptic = dataset_val\n",
        "datasets_te_elliptic = datasets_te\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eFFwPb4gjY-"
      },
      "source": [
        "#utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE5hldQxgyCa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os.path as osp\n",
        "from torch_geometric.datasets import Planetoid, PPI, WikiCS, Coauthor, Amazon, CoraFull\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "from torch_geometric.utils import add_remaining_self_loops, to_undirected\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.utils import subgraph\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def get_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), 'data', name)\n",
        "    if name in ['cora', 'citeseer', 'pubmed']:\n",
        "        dataset = Planetoid(path, name)\n",
        "    elif name in ['arxiv']:\n",
        "        dataset = PygNodePropPredDataset(name='ogbn-'+name)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return to_inductive(dataset)\n",
        "\n",
        "def mask_to_index(index, size):\n",
        "    all_idx = np.arange(size)\n",
        "    return all_idx[index]\n",
        "\n",
        "def index_to_mask(index, size):\n",
        "    mask = torch.zeros((size, ), dtype=torch.bool)\n",
        "    mask[index] = 1\n",
        "    return mask\n",
        "\n",
        "def resplit(data):\n",
        "    n = data.x.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    idx_train, idx_val, idx_test = get_train_val_test(nnodes=n, val_size=0.2, test_size=0.2, stratify=data.y)\n",
        "\n",
        "    data.train_mask = index_to_mask(idx_train, n)\n",
        "    data.val_mask = index_to_mask(idx_val, n)\n",
        "    data.test_mask = index_to_mask(idx_test, n)\n",
        "\n",
        "\n",
        "def add_mask(data, dataset):\n",
        "    # for arxiv\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "    n = data.x.shape[0]\n",
        "    data.train_mask = index_to_mask(train_idx, n)\n",
        "    data.val_mask = index_to_mask(valid_idx, n)\n",
        "    data.test_mask = index_to_mask(test_idx, n)\n",
        "    data.y = data.y.squeeze()\n",
        "    data.edge_index = to_undirected(data.edge_index, data.num_nodes)\n",
        "\n",
        "def holdout_val(data):\n",
        "    \"\"\"hold out a seperate validation from the original validation\"\"\"\n",
        "    n = data.x.shape[0]\n",
        "    idx = np.arange(n)\n",
        "    idx_val = idx[data.val_mask]\n",
        "\n",
        "    val1, val2 = train_test_split(idx_val, random_state=None,\n",
        "                           train_size=0.8, test_size=0.2, stratify=data.y[idx_val])\n",
        "\n",
        "    data.val1_mask = index_to_mask(val1, n)\n",
        "    data.val2_mask = index_to_mask(val2, n)\n",
        "\n",
        "\n",
        "def to_inductive(dataset):\n",
        "    data = dataset[0]\n",
        "    add_mask(data, dataset)\n",
        "\n",
        "    def sub_to_inductive(data, mask):\n",
        "        new_data = Graph()\n",
        "        new_data.graph['edge_index'], _ = subgraph(mask, data.edge_index, None,\n",
        "                              relabel_nodes=True, num_nodes=data.num_nodes)\n",
        "        new_data.graph['num_nodes'] = mask.sum().item()\n",
        "        new_data.graph['node_feat'] = data.x[mask]\n",
        "        new_data.label = data.y[mask].unsqueeze(1)\n",
        "        return new_data\n",
        "    train_graph = sub_to_inductive(data, data.train_mask)\n",
        "    val_graph = sub_to_inductive(data, data.val_mask)\n",
        "    test_graph = sub_to_inductive(data, data.test_mask)\n",
        "    val_graph.test_mask = torch.tensor(np.ones(val_graph.graph['num_nodes'])).bool()\n",
        "    test_graph.test_mask = torch.tensor(np.ones(test_graph.graph['num_nodes'])).bool()\n",
        "    return [train_graph, val_graph, [test_graph]]\n",
        "\n",
        "class Graph:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.test_mask = None\n",
        "        self.label = None\n",
        "        self.graph = {'edge_index': None, 'node_feat': None, 'num_nodes': None}\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_acc(y_true, y_pred):\n",
        "    acc_list = []\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    y_pred = y_pred.argmax(dim=-1, keepdim=True).detach().cpu().numpy()\n",
        "    return (y_true == y_pred).sum() / y_true.shape[0]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_rocauc(y_true, y_pred):\n",
        "    \"\"\" adapted from ogb\n",
        "    https://github.com/snap-stanford/ogb/blob/master/ogb/nodeproppred/evaluate.py\"\"\"\n",
        "    rocauc_list = []\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    if y_true.shape[1] == 1:\n",
        "        # use the predicted class for single-class classification\n",
        "        y_pred = F.softmax(y_pred, dim=-1)[:,1].unsqueeze(1).cpu().numpy()\n",
        "    else:\n",
        "        y_pred = y_pred.detach().cpu().numpy()\n",
        "\n",
        "    for i in range(y_true.shape[1]):\n",
        "        # AUC is only defined when there is at least one positive data.\n",
        "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:\n",
        "            is_labeled = y_true[:, i] == y_true[:, i]\n",
        "            score = roc_auc_score(y_true[is_labeled, i], y_pred[is_labeled, i])\n",
        "\n",
        "            rocauc_list.append(score)\n",
        "\n",
        "    if len(rocauc_list) == 0:\n",
        "        raise RuntimeError(\n",
        "            'No positively labeled data available. Cannot compute ROC-AUC.')\n",
        "\n",
        "    return sum(rocauc_list)/len(rocauc_list)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_f1(y_true, y_pred):\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    y_pred = y_pred.argmax(dim=-1, keepdim=True).detach().cpu().numpy()\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    # macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    return f1\n",
        "\n",
        "\n",
        "\n",
        "def reset_args(args):\n",
        "    args.weight_decay = 1e-3\n",
        "    args.dropout = 0\n",
        "    if args.dataset in ['cora', 'amazon-photo']:\n",
        "        args.lr = 0.001\n",
        "        args.nlayers = 2\n",
        "        args.hidden = 32\n",
        "\n",
        "    elif args.dataset == 'ogb-arxiv':\n",
        "        if args.ood:\n",
        "            args.lr = 0.01\n",
        "            args.nlayers=5\n",
        "            args.hidden = 32\n",
        "            args.weight_decay = 0\n",
        "        else:\n",
        "            args.lr = 0.01\n",
        "            args.dropout=0.5\n",
        "            args.nlayers = 3\n",
        "            args.hidden = 256\n",
        "            args.weight_decay = 0\n",
        "    elif args.dataset == 'fb100':\n",
        "        args.lr = 0.01\n",
        "        args.nlayers = 2\n",
        "        args.hidden = 32\n",
        "    elif args.dataset == 'twitch-e':\n",
        "        args.lr = 0.01\n",
        "        args.nlayers = 2\n",
        "        args.hidden = 32\n",
        "    elif args.dataset in ['elliptic']:\n",
        "        args.lr = 0.01\n",
        "        args.nlayers = 5\n",
        "        args.hidden = 32\n",
        "        args.weight_decay = 0\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if args.tune == 0:\n",
        "        import pandas as pd\n",
        "        filename = PARAM_CSV_PATH\n",
        "        df = pd.read_csv(filename, delimiter=',')\n",
        "        df2 = df[(df.dataset == args.dataset) & (df.model == args.model)]\n",
        "        params = df2[['lr_feat', 'lr_adj', 'epoch', 'ratio']].values\n",
        "        if len(params) == 1:\n",
        "            args.lr_feat, args.lr_adj, args.epochs, args.ratio = params[0]\n",
        "            args.epochs = int(args.epochs)\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJKk2Zw_ucit"
      },
      "source": [
        "#base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPb8rMlgueo1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "use test data to calculate the loss in SRGNN\n",
        "\"\"\"\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy\n",
        "# from deeprobust.graph import utils\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch_geometric.utils import dropout_adj\n",
        "\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "\n",
        "\n",
        "    def fit_inductive(self, data, train_iters=1000, initialize=True, verbose=False, patience=100, **kwargs):\n",
        "        if initialize:\n",
        "            self.initialize()\n",
        "\n",
        "        self.train_data = data[0]\n",
        "        self.val_data = data[1]\n",
        "        self.test_data = data[2]\n",
        "        # By default, it is trained with early stopping on validation\n",
        "        self.train_with_early_stopping(train_iters, patience, verbose)\n",
        "\n",
        "    # def fit_with_val1_val2(self, pyg_data, train_iters=1000, initialize=True, verbose=False, **kwargs):\n",
        "    def fit_with_val(self, pyg_data, train_iters=1000, initialize=True, patience=100, verbose=False, **kwargs):\n",
        "        if initialize:\n",
        "            self.initialize()\n",
        "\n",
        "        self.data = pyg_data.to(self.device)\n",
        "        self.data.train_mask = self.data.train_mask + self.data.val1_mask\n",
        "        self.data.val_mask = self.data.val2_mask\n",
        "        self.train_with_early_stopping(train_iters, patience, verbose)\n",
        "\n",
        "    def train_with_early_stopping(self, train_iters, patience, verbose):\n",
        "        \"\"\"early stopping based on the validation loss\n",
        "        \"\"\"\n",
        "        if verbose:\n",
        "            print(f'=== training {self.name} model ===')\n",
        "        # optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "        train_data, val_data = self.train_data, self.val_data\n",
        "\n",
        "        early_stopping = patience\n",
        "        # best_loss_val = 100\n",
        "        best_acc_val = float('-inf')\n",
        "\n",
        "        if type(train_data) is not list:\n",
        "            x, y = train_data.graph['node_feat'].to(self.device), train_data.label.to(self.device)#.squeeze()\n",
        "            edge_index = train_data.graph['edge_index'].to(self.device)\n",
        "\n",
        "            x_val, y_val = val_data.graph['node_feat'].to(self.device), val_data.label.to(self.device)#.squeeze()\n",
        "            edge_index_val = val_data.graph['edge_index'].to(self.device)\n",
        "\n",
        "        for i in range(train_iters):\n",
        "            self.train()\n",
        "            optimizer.zero_grad()\n",
        "            if type(train_data) is not list:\n",
        "                if hasattr(self, 'dropedge') and self.dropedge != 0:\n",
        "                    edge_index, _ = dropout_adj(edge_index, p=self.dropedge)\n",
        "\n",
        "                output = self.forward(x, edge_index)\n",
        "                if self.args.dataset == 'elliptic':\n",
        "                    loss_train = self.sup_loss(y[train_data.mask], output[train_data.mask])\n",
        "                else:\n",
        "                    loss_train = self.sup_loss(y, output)\n",
        "            else:\n",
        "                loss_train = 0\n",
        "                for graph_id, dat in enumerate(train_data):\n",
        "                    x, y = dat.graph['node_feat'].to(self.device), dat.label.to(self.device)#.squeeze()\n",
        "                    edge_index = dat.graph['edge_index'].to(self.device)\n",
        "                    if hasattr(self, 'dropedge') and self.dropedge != 0:\n",
        "                        edge_index, _ = dropout_adj(edge_index, p=self.dropedge)\n",
        "                    output = self.forward(x, edge_index)\n",
        "                    if self.args.dataset == 'elliptic':\n",
        "                        loss_train += self.sup_loss(y[dat.mask], output[dat.mask])\n",
        "                    else:\n",
        "                        loss_train += self.sup_loss(y, output)\n",
        "\n",
        "                loss_train = loss_train / len(train_data)\n",
        "            loss_train.backward()\n",
        "            optimizer.step()\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "            if verbose and i % 10 == 0:\n",
        "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
        "\n",
        "            self.eval()\n",
        "            eval_func = self.eval_func\n",
        "            if self.args.dataset in ['ogb-arxiv']:\n",
        "                output = self.forward(x_val, edge_index_val)\n",
        "                acc_val = eval_func(y_val[val_data.test_mask], output[val_data.test_mask])\n",
        "            elif self.args.dataset in ['cora', 'amazon-photo', 'twitch-e']:\n",
        "                output = self.forward(x_val, edge_index_val)\n",
        "                acc_val = eval_func(y_val, output)\n",
        "            elif self.args.dataset in ['fb100']:\n",
        "                y_val, out_val = [], []\n",
        "                # for i, dataset in enumerate(val_data):\n",
        "                #     x_val = dataset.graph['node_feat'].to(self.device)\n",
        "                #     edge_index_val = dataset.graph['edge_index'].to(self.device)\n",
        "                #     out = self.forward(x_val, edge_index_val)\n",
        "                #     y_val.append(dataset.label.to(self.device))\n",
        "                #     out_val.append(out)\n",
        "\n",
        "                x_val = val_data.graph['node_feat'].to(self.device)\n",
        "                edge_index_val = val_data.graph['edge_index'].to(self.device)\n",
        "                out = self.forward(x_val, edge_index_val)\n",
        "                y_val.append(val_data.label.to(self.device))\n",
        "                out_val.append(out)\n",
        "\n",
        "                acc_val = eval_func(torch.cat(y_val, dim=0), torch.cat(out_val, dim=0))\n",
        "            elif self.args.dataset in ['elliptic']:\n",
        "                # acc_val = eval_func(y_val, output)\n",
        "                y_val, out_val = [], []\n",
        "                # for i, dataset in enumerate(val_data):\n",
        "                #     x_val = dataset.graph['node_feat'].to(self.device)\n",
        "                #     edge_index_val = dataset.graph['edge_index'].to(self.device)\n",
        "                #     out = self.forward(x_val, edge_index_val)\n",
        "                #     y_val.append(dataset.label[dataset.mask].to(self.device))\n",
        "                #     out_val.append(out[dataset.mask])\n",
        "\n",
        "                x_val = val_data.graph['node_feat'].to(self.device)\n",
        "                edge_index_val = val_data.graph['edge_index'].to(self.device)\n",
        "                out = self.forward(x_val, edge_index_val)\n",
        "                y_val.append(val_data.label[val_data.mask].to(self.device))\n",
        "                out_val.append(out[val_data.mask])\n",
        "\n",
        "                acc_val = eval_func(torch.cat(y_val, dim=0), torch.cat(out_val, dim=0))\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            if best_acc_val < acc_val:\n",
        "                best_acc_val = acc_val\n",
        "                self.output = output\n",
        "                weights = deepcopy(self.state_dict())\n",
        "                patience = early_stopping\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if i > early_stopping and patience <= 0:\n",
        "                break\n",
        "\n",
        "        if verbose:\n",
        "             # print('=== early stopping at {0}, loss_val = {1} ==='.format(i, best_loss_val) )\n",
        "             print('=== early stopping at {0}, acc_val = {1} ==='.format(i, best_acc_val) )\n",
        "        self.load_state_dict(weights)\n",
        "\n",
        "    def sup_loss(self, y, pred):\n",
        "        if self.args.dataset in ('twitch-e', 'fb100', 'elliptic'):\n",
        "            if y.shape[1] == 1:\n",
        "                true_label = F.one_hot(y, y.max() + 1).squeeze(1)\n",
        "            else:\n",
        "                true_label = y\n",
        "            criterion = nn.BCEWithLogitsLoss()\n",
        "            loss = criterion(pred, true_label.squeeze(1).to(torch.float))\n",
        "        else:\n",
        "            out = F.log_softmax(pred, dim=1)\n",
        "            target = y.squeeze(1)\n",
        "            criterion = nn.NLLLoss()\n",
        "            loss = criterion(out, target)\n",
        "        return loss\n",
        "\n",
        "    def get_pred(self, logits):\n",
        "        if self.args.dataset in ('twitch-e', 'fb100', 'elliptic'):\n",
        "            pred = torch.sigmoid(logits)\n",
        "        else:\n",
        "            pred = F.softmax(logits, dim=1)\n",
        "        return pred\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Evaluate model performance on test set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx_test :\n",
        "            node testing indices\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        test_mask = self.data.test_mask\n",
        "        labels = self.data.y\n",
        "        output = self.forward(self.data.x, self.data.edge_index)\n",
        "        # output = self.output\n",
        "        loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
        "        # acc_test = utils.accuracy(output[test_mask], labels[test_mask])\n",
        "        acc_test = (output[test_mask] == labels[test_mask]).float().mean()\n",
        "        print(\"Test set results:\",\n",
        "              \"loss= {:.4f}\".format(loss_test.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "        return acc_test.item()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, x=None, edge_index=None, edge_weight=None):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            output (log probabilities)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        if x is None or edge_index is None:\n",
        "            x, edge_index = self.test_data.graph['node_feat'], self.test_data.graph['edge_index']\n",
        "            x, edge_index = x.to(self.device), edge_index.to(self.device)\n",
        "        return self.forward(x, edge_index, edge_weight)\n",
        "\n",
        "    def _ensure_contiguousness(self,\n",
        "                               x,\n",
        "                               edge_idx,\n",
        "                               edge_weight):\n",
        "        if not x.is_sparse:\n",
        "            x = x.contiguous()\n",
        "        if hasattr(edge_idx, 'contiguous'):\n",
        "            edge_idx = edge_idx.contiguous()\n",
        "        if edge_weight is not None:\n",
        "            edge_weight = edge_weight.contiguous()\n",
        "        return x, edge_idx, edge_weight\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S6XH80UuSDo"
      },
      "source": [
        "# GCN.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8-HQ3xouUYn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_sparse import coalesce, SparseTensor\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class GCN(BaseModel):\n",
        "\n",
        "    def __init__(self, nfeat, nhid, nclass, nlayers=2, dropout=0.5, lr=0.01, save_mem=True,\n",
        "                with_bn=False, weight_decay=5e-4, with_bias=True, device=None, args=None):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        assert device is not None, \"Please specify 'device'!\"\n",
        "        self.device = device\n",
        "        self.args = args\n",
        "        # from utils import eval_acc, eval_f1, eval_rocauc\n",
        "        if args.dataset == 'twitch-e':\n",
        "            self.eval_func = eval_rocauc\n",
        "        elif args.dataset == 'elliptic':\n",
        "            self.eval_func = eval_f1\n",
        "        elif args.dataset in ['cora', 'amazon-photo', 'ogb-arxiv', 'fb100']:\n",
        "            self.eval_func = eval_acc\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        if with_bn:\n",
        "            self.bns = nn.ModuleList()\n",
        "\n",
        "        if nlayers == 1:\n",
        "            self.layers.append(GCNConv(nfeat, nclass, bias=with_bias, normalize=not save_mem))\n",
        "        else:\n",
        "            self.layers.append(GCNConv(nfeat, nhid, bias=with_bias, normalize=not save_mem))\n",
        "            if with_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(nhid))\n",
        "            for i in range(nlayers-2):\n",
        "                self.layers.append(GCNConv(nhid, nhid, bias=with_bias, normalize=not save_mem))\n",
        "                if with_bn:\n",
        "                    self.bns.append(nn.BatchNorm1d(nhid))\n",
        "            self.layers.append(GCNConv(nhid, nclass, bias=with_bias, normalize=not save_mem))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lr = lr\n",
        "        self.output = None\n",
        "        self.best_model = None\n",
        "        self.best_output = None\n",
        "        self.with_bn = with_bn\n",
        "        self.name = 'GCN'\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, dropout_rate=0.0):\n",
        "        x, edge_index, edge_weight = self._ensure_contiguousness(x, edge_index, edge_weight)\n",
        "        if edge_weight is not None:\n",
        "            adj = SparseTensor.from_edge_index(edge_index, edge_weight, sparse_sizes=2 * x.shape[:1]).t()\n",
        "\n",
        "        for ii, layer in enumerate(self.layers):\n",
        "            if edge_weight is not None:\n",
        "                x = layer(x, adj)\n",
        "            else:\n",
        "                # x = layer(x, edge_index, edge_weight=edge_weight)\n",
        "                x = layer(x, edge_index)\n",
        "            if ii != len(self.layers) - 1:\n",
        "                if self.with_bn:\n",
        "                    x = self.bns[ii](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "                x = F.dropout(x, p=dropout_rate, training=True)\n",
        "                self.h = x # TODO\n",
        "        return x\n",
        "        # return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def get_embed(self, x, edge_index, edge_weight=None):\n",
        "        x, edge_index, edge_weight = self._ensure_contiguousness(x, edge_index, edge_weight)\n",
        "        for ii, layer in enumerate(self.layers):\n",
        "            if ii == len(self.layers) - 1:\n",
        "                return x\n",
        "            if edge_weight is not None:\n",
        "                adj = SparseTensor.from_edge_index(edge_index, edge_weight,\n",
        "                        sparse_sizes=2 * x.shape[:1]).t() # in case it is directed...\n",
        "\n",
        "                # layer(x, edge_index, edge_weight)\n",
        "                x = layer(x, adj)\n",
        "            else:\n",
        "                x = layer(x, edge_index)\n",
        "            if ii != len(self.layers) - 1:\n",
        "                if self.with_bn:\n",
        "                    x = self.bns[ii](x)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "    def initialize(self):\n",
        "        for m in self.layers:\n",
        "            m.reset_parameters()\n",
        "        if self.with_bn:\n",
        "            for bn in self.bns:\n",
        "                bn.reset_parameters()\n",
        "\n",
        "    def setup_dae(self, nfeat, nhid, nclass):\n",
        "        self.dae_layers = nn.ModuleList([])\n",
        "        self.dae_layers.append(GCNConv(nfeat, nhid))\n",
        "        self.dae_layers.append(GCNConv(nhid, nclass))\n",
        "        for m in self.dae_layers:\n",
        "            m.reset_parameters()\n",
        "        return\n",
        "\n",
        "    def train_dae(self, x, edge_index, edge_weight):\n",
        "        x, edge_index = x.to(self.device), edge_index.to(self.device)\n",
        "        optimizer = optim.Adam(self.dae_layers.parameters(), lr=0.01, weight_decay=0)\n",
        "        epochs = 50\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            optimizer.zero_grad()\n",
        "            loss = self.get_loss_masked_features(x, edge_index, edge_weight)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if args.verbose:\n",
        "              print(\"Epoch {:05d} | Train Loss {:.4f}\".format(epoch, loss.item()))\n",
        "        return\n",
        "\n",
        "    def get_loss_masked_features(self, features, edge_index, edge_weight):\n",
        "        ratio = 10 #; nr = 5\n",
        "        # noise = 'mask'\n",
        "        noise = 'normal'\n",
        "        def get_random_mask_ogb(features, r):\n",
        "            probs = torch.full(features.shape, 1/r)\n",
        "            mask = torch.bernoulli(probs)\n",
        "            return mask\n",
        "\n",
        "        mask = get_random_mask_ogb(features, ratio).cuda()\n",
        "        if noise == 'mask':\n",
        "            masked_features = features * (1 - mask)\n",
        "        elif noise == \"normal\":\n",
        "            noise = torch.normal(0.0, 1.0, size=features.shape).cuda()\n",
        "            masked_features = features + (noise * mask)\n",
        "\n",
        "        self.dae_layers = self.dae_layers.to(self.device)\n",
        "        x = features\n",
        "        for layer in self.dae_layers[:-1]:\n",
        "            x = layer(x, edge_index, edge_weight)\n",
        "            x = F.relu(x)\n",
        "            # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        from torch_sparse import coalesce, SparseTensor\n",
        "        adj = SparseTensor.from_edge_index(edge_index, edge_weight, sparse_sizes=2 * x.shape[:1]).t()\n",
        "        x = self.dae_layers[-1](x, adj)\n",
        "        # x = self.dae_layers[-1](x, edge_index, edge_weight)\n",
        "        logits = x\n",
        "        indices = mask > 0\n",
        "        loss = F.mse_loss(logits[indices], features[indices], reduction='mean')\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQbtKiqvIBwV"
      },
      "source": [
        "# GAT.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljbq8zNQH-ye"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_sparse import coalesce, SparseTensor\n",
        "\n",
        "\n",
        "class GAT(BaseModel):\n",
        "\n",
        "    def __init__(self, nfeat, nhid, nclass, heads=8, output_heads=1, dropout=0., lr=0.01,\n",
        "            nlayers=2, with_bn=False, weight_decay=5e-4, device=None, args=None):\n",
        "\n",
        "        super(GAT, self).__init__()\n",
        "        # from utils import eval_acc, eval_f1, eval_rocauc\n",
        "        if args.dataset == 'twitch-e':\n",
        "            self.eval_func = eval_rocauc\n",
        "        elif args.dataset == 'elliptic':\n",
        "            self.eval_func = eval_f1\n",
        "        elif args.dataset in ['cora', 'amazon-photo', 'ogb-arxiv', 'fb100']:\n",
        "            self.eval_func = eval_acc\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        assert device is not None, \"Please specify 'device'!\"\n",
        "        self.device = device\n",
        "\n",
        "        if with_bn:\n",
        "            self.bns = nn.ModuleList()\n",
        "            self.bns.append(nn.BatchNorm1d(nhid*heads))\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GATConv(\n",
        "                nfeat,\n",
        "                nhid,\n",
        "                heads=heads,\n",
        "                dropout=dropout))\n",
        "\n",
        "        for _ in range(nlayers - 2):\n",
        "            self.convs.append(GATConv(\n",
        "                    nhid * heads,\n",
        "                    nhid,\n",
        "                    heads=heads,\n",
        "                    dropout=dropout))\n",
        "            if with_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(nhid*heads))\n",
        "\n",
        "        self.convs.append(GATConv(\n",
        "            nhid * heads,\n",
        "            nclass,\n",
        "            heads=output_heads,\n",
        "            concat=False,\n",
        "            dropout=dropout))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lr = lr\n",
        "        self.output = None\n",
        "        self.best_model = None\n",
        "        self.best_output = None\n",
        "        self.activation = F.elu\n",
        "        self.name = 'GAT'\n",
        "        self.args = args\n",
        "        self.with_bn = with_bn\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, dropout_rate=0.0):\n",
        "        if edge_weight is not None:\n",
        "            adj = SparseTensor.from_edge_index(edge_index, edge_weight, sparse_sizes=2 * x.shape[:1]).t()\n",
        "\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            if edge_weight is not None:\n",
        "                x = conv(x, adj)\n",
        "            else:\n",
        "                x = conv(x, edge_index, edge_weight)\n",
        "            if self.with_bn:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = F.dropout(x, p=dropout_rate, training=True) # add for dropout inference\n",
        "\n",
        "        if edge_weight is not None:\n",
        "            x = self.convs[-1](x, adj)\n",
        "        else:\n",
        "            x = self.convs[-1](x, edge_index, edge_weight)\n",
        "        # return F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def initialize(self):\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def get_embed(self, x, edge_index, edge_weight=None):\n",
        "        x, edge_index, edge_weight = self._ensure_contiguousness(x, edge_index, edge_weight)\n",
        "        for ii, layer in enumerate(self.convs):\n",
        "            if ii == len(self.convs) - 1:\n",
        "                return x\n",
        "            if edge_weight is not None:\n",
        "                adj = SparseTensor.from_edge_index(edge_index, edge_weight, sparse_sizes=2 * x.shape[:1]).t()\n",
        "                x = layer(x, adj)\n",
        "            else:\n",
        "                x = layer(x, edge_index)\n",
        "            if ii != len(self.convs) - 1:\n",
        "                if self.with_bn:\n",
        "                    x = self.bns[ii](x)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_nE-IkmIG2J"
      },
      "source": [
        "# SAGE.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c57blJEqIOBA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "# from torch_geometric.nn import SAGEConv, GATConv, APPNP, MessagePassing\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "import scipy.sparse\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SAGE(BaseModel):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2,\n",
        "                 dropout=0.5, lr=0.01, weight_decay=0, device='cpu', with_bn=True, args=None):\n",
        "        super(SAGE, self).__init__()\n",
        "        # from utils import eval_acc, eval_f1, eval_rocauc\n",
        "        if args.dataset == 'twitch-e':\n",
        "            self.eval_func = eval_rocauc\n",
        "        elif args.dataset == 'elliptic':\n",
        "            self.eval_func = eval_f1\n",
        "        elif args.dataset in ['cora', 'amazon-photo', 'ogb-arxiv', 'fb100']:\n",
        "            self.eval_func = eval_acc\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.args = args\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            SAGEConv(in_channels, hidden_channels))\n",
        "\n",
        "        self.bns = nn.ModuleList()\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                SAGEConv(hidden_channels, hidden_channels))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.convs.append(\n",
        "            SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lr = lr\n",
        "        self.dropout = dropout\n",
        "        self.activation = F.relu\n",
        "        self.with_bn = with_bn\n",
        "        self.device = device\n",
        "        self.name = \"SAGE2\"\n",
        "\n",
        "    def initialize(self):\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def get_embed(self, x, edge_index, edge_weight=None):\n",
        "        x, edge_index, edge_weight = self._ensure_contiguousness(x, edge_index, edge_weight)\n",
        "        for ii, layer in enumerate(self.convs):\n",
        "            if ii == len(self.convs) - 1:\n",
        "                return x\n",
        "            if edge_weight is not None:\n",
        "                adj = SparseTensor.from_edge_index(edge_index, edge_weight, sparse_sizes=2 * x.shape[:1]).t()\n",
        "                x = layer(x, adj)\n",
        "            else:\n",
        "                x = layer(x, edge_index)\n",
        "            if ii != len(self.convs) - 1:\n",
        "                if self.with_bn:\n",
        "                    x = self.bns[ii](x)\n",
        "                x = F.relu(x)\n",
        "                # x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None, dropout_rate=0.0):\n",
        "        if edge_weight is not None:\n",
        "            adj = SparseTensor.from_edge_index(edge_index, edge_weight, sparse_sizes=2 * x.shape[:1]).t()\n",
        "\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            if edge_weight is not None:\n",
        "                x = conv(x, adj)\n",
        "            else:\n",
        "                x = conv(x, edge_index, edge_weight)\n",
        "            if self.with_bn:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            x = F.dropout(x, p=dropout_rate, training=True) # add dropout inferences\n",
        "        if edge_weight is not None:\n",
        "            x = self.convs[-1](x, adj)\n",
        "        else:\n",
        "            x = self.convs[-1](x, edge_index, edge_weight)\n",
        "        # return F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "from typing import Union, Tuple\n",
        "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "\n",
        "class SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
        "                 out_channels: int, normalize: bool = False,\n",
        "                 bias: bool = True, **kwargs):  # yapf: disable\n",
        "        kwargs.setdefault('aggr', 'mean')\n",
        "        super(SAGEConv, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "\n",
        "        if isinstance(in_channels, int):\n",
        "            in_channels = (in_channels, in_channels)\n",
        "\n",
        "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
        "        self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        self.lin_r.reset_parameters()\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                size: Size = None) -> Tensor:\n",
        "        \"\"\"\"\"\"\n",
        "        if 0:\n",
        "            if isinstance(x, Tensor):\n",
        "                x: OptPairTensor = (x, x)\n",
        "            # propagate_type: (x: OptPairTensor)\n",
        "            out = self.propagate(edge_index, x=x, size=size)\n",
        "            out = self.lin_l(out)\n",
        "        else: # for  fb100 dataset\n",
        "            if isinstance(x, Tensor):\n",
        "                x: OptPairTensor = (x, x)\n",
        "            out = self.lin_l(x[0])\n",
        "            # propagate_type: (x: OptPairTensor)\n",
        "            out = self.propagate(edge_index, x=(out, out), size=size)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out += self.lin_r(x_r)\n",
        "\n",
        "        if self.normalize:\n",
        "            out = F.normalize(out, p=2., dim=-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j: Tensor) -> Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor,\n",
        "                              x: OptPairTensor) -> Tensor:\n",
        "        # Deleted the following line to make propagation differentiable\n",
        "        # adj_t = adj_t.set_value(None, layout=None)\n",
        "        return matmul(adj_t, x[0], reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
        "                                   self.out_channels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFV3-GhH0b8R"
      },
      "source": [
        "#FeatAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXIQ-c2oglkv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# from models import *\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "# import deeprobust.graph.utils as utils\n",
        "from torch.nn.parameter import Parameter\n",
        "from tqdm import tqdm\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "# from utils import reset_args\n",
        "import random\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix, dropout_adj, is_undirected, to_undirected\n",
        "\n",
        "class FeatAgent:\n",
        "\n",
        "    def __init__(self, data_all, args):\n",
        "        self.device = 'cuda'\n",
        "        self.args = args\n",
        "        self.data_all = data_all\n",
        "        self.model = self.pretrain_model(verbose=args.verbose)\n",
        "\n",
        "    def initialize_as_ori_feat(self, feat):\n",
        "        self.delta_feat.data.copy_(feat)\n",
        "\n",
        "    def finetune(self, data):\n",
        "        args = self.args\n",
        "        if self.args.verbose:\n",
        "          print('Finetuning ...')\n",
        "\n",
        "        for module in self.model.bns.modules():\n",
        "            if isinstance(module, torch.nn.BatchNorm1d) or isinstance(module, torch.nn.BatchNorm2d):\n",
        "                if args.use_learned_stats:\n",
        "                    module.track_running_stats = True\n",
        "                    module.momentum = args.bn_momentum\n",
        "                else:\n",
        "                    module.track_running_stats = False\n",
        "                    module.running_mean = None\n",
        "                    module.running_var = None\n",
        "\n",
        "        if not hasattr(self, 'model_pre_state'):\n",
        "            self.model_pre_state = deepcopy(self.model.state_dict())\n",
        "        if not args.not_reset:\n",
        "            self.model.load_state_dict(self.model_pre_state) # reset every test sample\n",
        "        assert args.debug == 1 or args.debug == 2\n",
        "        model = self.model\n",
        "\n",
        "        if args.tent:\n",
        "            for param in model.parameters():\n",
        "                if args.train_all:\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "            for module in model.bns.modules():\n",
        "                if isinstance(module, torch.nn.BatchNorm1d) or isinstance(module, torch.nn.BatchNorm2d):\n",
        "                    if args.use_learned_stats:\n",
        "                        module.track_running_stats = True\n",
        "                        module.momentum = args.bn_momentum\n",
        "                    else:\n",
        "                        module.track_running_stats = False\n",
        "                        module.running_mean = None\n",
        "                        module.running_var = None\n",
        "                    module.weight.requires_grad_(True)\n",
        "                    module.bias.requires_grad_(True)\n",
        "            # for param in model.bns.parameters():\n",
        "            #     param.requires_grad = True\n",
        "\n",
        "            if args.sam:\n",
        "                args.loss = 'sharpness'\n",
        "                optimizer = SAM(model.parameters(), torch.optim.Adam, lr=args.lr_tta, weight_decay=0)\n",
        "            else:\n",
        "                args.loss = 'entropy'\n",
        "                optimizer = optim.Adam(model.parameters(), lr=args.lr_tta, weight_decay=0)\n",
        "        else:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "            args.lr = args.lr_feat\n",
        "            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0)\n",
        "            # args.lr =0.001; args.epochs=10\n",
        "\n",
        "        # model.lr=0.1\n",
        "        edge_index = data.graph['edge_index'].to(self.device)\n",
        "        feat, labels = data.graph['node_feat'].to(self.device), data.label.to(self.device) #.squeeze()\n",
        "\n",
        "        self.feat, self.data = feat, data\n",
        "\n",
        "        # print(labels)\n",
        "        model.train()\n",
        "        # for i in range(args.epochs):\n",
        "        do_bw = args.sam\n",
        "        for i in range(args.ep_tta):\n",
        "            optimizer.zero_grad()\n",
        "            loss = self.test_time_loss(model, feat, edge_index, do_bw=do_bw, optimizer=optimizer)\n",
        "            if not do_bw:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            if i == 0:\n",
        "              if self.args.verbose:\n",
        "                print(f'Epoch {i}: {loss}')\n",
        "\n",
        "        model.eval()\n",
        "        output = model.predict(feat, edge_index)\n",
        "        loss = self.test_time_loss(model, feat, edge_index, do_bw = False)\n",
        "        if self.args.verbose:\n",
        "          print(f'Epoch {i}: {loss}')\n",
        "          print('Test:')\n",
        "\n",
        "        if args.dataset == 'elliptic':\n",
        "            return self.evaluate_single(model, output, labels, data), output[data.mask], labels[data.mask]\n",
        "        else:\n",
        "            return self.evaluate_single(model, output, labels, data), output, labels\n",
        "\n",
        "    # will re re-wrote by other classes\n",
        "    def learn_graph(self, data):\n",
        "        args = self.args\n",
        "        args = self.args\n",
        "        self.data = data\n",
        "        nnodes = data.graph['node_feat'].shape[0]\n",
        "        d = data.graph['node_feat'].shape[1]\n",
        "        # optimize delta_feat !!\n",
        "        delta_feat = Parameter(torch.FloatTensor(nnodes, d).to(self.device))\n",
        "        self.delta_feat = delta_feat\n",
        "        delta_feat.data.fill_(1e-7)\n",
        "        self.optimizer_feat = torch.optim.Adam([delta_feat], lr=args.lr_feat)\n",
        "\n",
        "        # not learned model anymore, just transform input\n",
        "        model = self.model\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        model.eval() # should set to eval\n",
        "\n",
        "        feat, labels = data.graph['node_feat'].to(self.device), data.label.to(self.device)#.squeeze()\n",
        "        edge_index = data.graph['edge_index'].to(self.device)\n",
        "        self.edge_index, self.feat, self.labels = edge_index, feat, labels\n",
        "\n",
        "        for it in tqdm(range(args.epochs)):\n",
        "            self.optimizer_feat.zero_grad()\n",
        "            loss = self.test_time_loss(model, feat+delta_feat, edge_index)\n",
        "\n",
        "            loss.backward()\n",
        "            if it % 100 == 0:\n",
        "              if self.args.verbose:\n",
        "                print(f'Epoch {it}: {loss}')\n",
        "\n",
        "            self.optimizer_feat.step()\n",
        "            if args.debug==2:\n",
        "                output = model.predict(feat+delta_feat, edge_index)\n",
        "                if self.args.verbose:\n",
        "                  print('Test:', self.evaluate_single(model, output, labels, data))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss = self.test_time_loss(model, feat+delta_feat, edge_index)\n",
        "        if self.args.verbose:\n",
        "          print(f'Epoch {it+1}: {loss}')\n",
        "\n",
        "        output = model.predict(feat+delta_feat, edge_index)\n",
        "        if self.args.verbose:\n",
        "          print('Test on transformed graph:')\n",
        "        if args.dataset == 'elliptic':\n",
        "            return self.evaluate_single(model, output, labels, data), output[data.mask], labels[data.mask]\n",
        "        else:\n",
        "            return self.evaluate_single(model, output, labels, data), output, labels\n",
        "\n",
        "    def augment(self, strategy='dropedge', p=0.5, edge_index=None, edge_weight=None):\n",
        "        model = self.model\n",
        "        if hasattr(self, 'delta_feat'):\n",
        "            delta_feat = self.delta_feat\n",
        "            feat = self.feat + delta_feat\n",
        "        else:\n",
        "            feat = self.feat\n",
        "        if strategy == 'shuffle':\n",
        "            idx = np.random.permutation(feat.shape[0])\n",
        "            shuf_fts = feat[idx, :]\n",
        "            output = model.get_embed(shuf_fts, edge_index, edge_weight)\n",
        "        if strategy == \"dropedge\":\n",
        "            edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"dropnode\":\n",
        "            feat = self.feat + self.delta_feat\n",
        "            mask = torch.cuda.FloatTensor(len(feat)).uniform_() > p\n",
        "            feat = feat * mask.view(-1, 1)\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"dropmix\":\n",
        "            feat = self.feat + self.delta_feat\n",
        "            mask = torch.cuda.FloatTensor(len(feat)).uniform_() > p\n",
        "            feat = feat * mask.view(-1, 1)\n",
        "            edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"dropfeat\":\n",
        "            feat = F.dropout(self.feat, p=p) + self.delta_feat\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"featnoise\":\n",
        "            mean, std = 0, p\n",
        "            noise = torch.randn(feat.size()) * std + mean\n",
        "            feat = feat + noise.to(feat.device)\n",
        "            output = model.get_embed(feat, edge_index)\n",
        "        return output\n",
        "\n",
        "    def tta_augment(self, strategy='dropedge', p=0.5, edge_index=None, edge_weight=None):\n",
        "        model = self.model\n",
        "        if hasattr(self, 'delta_feat'):\n",
        "            delta_feat = self.delta_feat\n",
        "            feat = self.feat + delta_feat\n",
        "        else:\n",
        "            feat = self.feat\n",
        "        if strategy == 'shuffle':\n",
        "            idx = np.random.permutation(feat.shape[0])\n",
        "            shuf_fts = feat[idx, :]\n",
        "            # output = model.get_embed(shuf_fts, edge_index, edge_weight)\n",
        "            output = model.forward(shuf_fts, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"dropedge\":\n",
        "            edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            # output = model.get_embed(feat, edge_index, edge_weight)\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"dropnode\":\n",
        "            feat = self.feat + self.delta_feat\n",
        "            mask = torch.cuda.FloatTensor(len(feat)).uniform_() > p\n",
        "            feat = feat * mask.view(-1, 1)\n",
        "            # output = model.get_embed(feat, edge_index, edge_weight)\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"dropmix\":\n",
        "            feat = self.feat + self.delta_feat\n",
        "            mask = torch.cuda.FloatTensor(len(feat)).uniform_() > p\n",
        "            feat = feat * mask.view(-1, 1)\n",
        "            edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            # output = model.get_embed(feat, edge_index, edge_weight)\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"dropfeat\":\n",
        "            feat = F.dropout(self.feat, p=p) + self.delta_feat\n",
        "            # output = model.get_embed(feat, edge_index, edge_weight)\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"featnoise\":\n",
        "            mean, std = 0, p\n",
        "            noise = torch.randn(feat.size()) * std + mean\n",
        "            feat = feat + noise.to(feat.device)\n",
        "            # output = model.get_embed(feat, edge_index)\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "\n",
        "        return output\n",
        "\n",
        "    # loss during test time : can be to propogate to model or data (depends on type of loss)\n",
        "    def test_time_loss(self, model, feat, edge_index, edge_weight=None, mode='train', do_bw=False, optimizer=None):\n",
        "        args = self.args\n",
        "        loss = 0\n",
        "        # print(args.loss)\n",
        "        if 'LC' in args.loss: # label constitency\n",
        "            if mode == 'eval': # random seed setting\n",
        "                random.seed(args.seed)\n",
        "                np.random.seed(args.seed)\n",
        "                torch.manual_seed(args.seed)\n",
        "                torch.cuda.manual_seed(args.seed)\n",
        "            if args.strategy == 'dropedge':\n",
        "                # output1 = self.augment(strategy=args.strategy, p=0.5, edge_index=edge_index, edge_weight=edge_weight)\n",
        "                output1 = self.augment(strategy=args.strategy, p=0.05, edge_index=edge_index, edge_weight=edge_weight) #TODO\n",
        "            if args.strategy == 'dropnode':\n",
        "                output1 = self.augment(strategy=args.strategy, p=0.05, edge_index=edge_index, edge_weight=edge_weight)\n",
        "            if args.strategy == 'rwsample':\n",
        "                output1 = self.augment(strategy=args.strategy, edge_index=edge_index, edge_weight=edge_weight)\n",
        "            output2 = self.augment(strategy='dropedge', p=0.0, edge_index=edge_index, edge_weight=edge_weight)\n",
        "            output3 = self.augment(strategy='shuffle', edge_index=edge_index, edge_weight=edge_weight)\n",
        "            if args.margin != -1:\n",
        "                loss = inner(output1, output2) - inner_margin(output2, output3, margin=args.margin)\n",
        "            else:\n",
        "                loss = inner(output1, output2) - inner(output2, output3)\n",
        "\n",
        "        if 'recon' in args.loss: # data reconstruction\n",
        "            model = self.model\n",
        "            delta_feat = self.delta_feat\n",
        "            feat = self.feat + delta_feat\n",
        "            output2 = model.get_embed(feat, edge_index, edge_weight)\n",
        "            loss += inner(output2[edge_index[0]], output2[edge_index[1]])\n",
        "\n",
        "        if args.loss == \"train\":\n",
        "            train_mask = self.data.train_mask\n",
        "            loss = F.nll_loss(output[train_mask], labels[train_mask])\n",
        "\n",
        "        if args.loss == \"test\":\n",
        "            model, data = self.model, self.data\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "            y = data.label.to(self.device)\n",
        "            if self.args.dataset == 'elliptic':\n",
        "                loss = model.sup_loss(y[data.mask], output[data.mask])\n",
        "            elif args.dataset == 'ogb-arxiv':\n",
        "                loss = model.sup_loss(y[data.test_mask], output[data.test_mask])\n",
        "            else:\n",
        "                loss = model.sup_loss(y, output)\n",
        "\n",
        "        if \"entropy\" in args.loss: # TTA (TENT)\n",
        "            model, data = self.model, self.data\n",
        "            feat = self.feat\n",
        "            batch_size = 1000\n",
        "            if args.aug > 0:\n",
        "              output = []\n",
        "              for _ in range(args.aug):\n",
        "                output += [self.tta_augment(strategy=args.strategy, p=0.05, edge_index=edge_index, edge_weight=edge_weight)]\n",
        "              output = torch.cat(output, axis = 0)\n",
        "            elif args.dropout_inference > 0:\n",
        "              output = []\n",
        "              for _ in range(args.dropout_inference):\n",
        "                output += [model.forward(feat, edge_index, edge_weight, dropout_rate=args.dropout_rate)]\n",
        "              output = torch.stack(output, dim=1)\n",
        "              output = torch.mean(output, dim=1)\n",
        "            else:\n",
        "              output = model.forward(feat, edge_index, edge_weight)\n",
        "            entropy = softmax_entropy(output)\n",
        "            if args.ent_filter != None:\n",
        "                mask = entropy < args.ent_filter\n",
        "                selected_entropy = entropy[mask]\n",
        "                if self.args.verbose:\n",
        "                  print(f\"num selected ent : {len(selected_entropy)}\")\n",
        "                  print(f\"total ent : {len(entropy)}\")\n",
        "            else:\n",
        "                selected_entropy = entropy\n",
        "            loss = selected_entropy.mean(0)\n",
        "\n",
        "        if \"sharpness\" == args.loss:\n",
        "            model, data = self.model, self.data\n",
        "            if hasattr(self, 'delta_feat'):\n",
        "                delta_feat = self.delta_feat\n",
        "                feat = self.feat + delta_feat\n",
        "            else:\n",
        "                feat = self.feat\n",
        "            batch_size = 1000\n",
        "            output = model.forward(feat, edge_index, edge_weight)\n",
        "            entropy = softmax_entropy(output)\n",
        "            if args.ent_filter != None:\n",
        "                mask = entropy < args.ent_filter\n",
        "                selected_entropy = entropy[mask]\n",
        "                if self.args.verbose:\n",
        "                  print(f\"num selected ent : {len(selected_entropy)}\")\n",
        "                  print(f\"total ent : {len(entropy)}\")\n",
        "            else:\n",
        "                selected_entropy = entropy\n",
        "\n",
        "            loss = selected_entropy.mean(0)\n",
        "\n",
        "            if do_bw:\n",
        "                loss.backward()\n",
        "\n",
        "                # compute \\hat{\\epsilon(\\Theta)} for first order approximation, Eqn. (4)\n",
        "                optimizer.first_step(zero_grad=True)\n",
        "\n",
        "                # second time backward, update model weights using gradients at \\Theta+\\hat{\\epsilon(\\Theta)}\n",
        "                output = model.forward(feat, edge_index, edge_weight)\n",
        "                entropy2 = softmax_entropy(output)\n",
        "\n",
        "                if args.ent_filter != None:\n",
        "                    mask = entropy2 < args.ent_filter\n",
        "                    selected_entropy2 = entropy2[mask]\n",
        "                    if self.args.verbose:\n",
        "                      print(f\"num selected ent2 : {len(selected_entropy2)}\")\n",
        "                      print(f\"total ent2 : {len(entropy2)}\")\n",
        "                else:\n",
        "                    selected_entropy2 = entropy2\n",
        "\n",
        "                loss2 = selected_entropy2.mean(0)\n",
        "                loss2.backward()\n",
        "\n",
        "                optimizer.second_step(zero_grad=False)\n",
        "\n",
        "        if args.loss == 'dae':\n",
        "            if hasattr(self, 'delta_feat'):\n",
        "                delta_feat = self.delta_feat\n",
        "                feat = self.feat + delta_feat\n",
        "            else:\n",
        "                feat = self.feat\n",
        "            loss = model.get_loss_masked_features(feat, edge_index, edge_weight)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def pretrain_model(self, verbose=True):\n",
        "        data_all = self.data_all\n",
        "        args = self.args\n",
        "        device = self.device\n",
        "        if type(data_all[0]) is not list:\n",
        "            feat, labels = data_all[0].graph['node_feat'], data_all[0].label\n",
        "            edge_index = data_all[0].graph['edge_index']\n",
        "        else:\n",
        "            feat, labels = data_all[0][0].graph['node_feat'], data_all[0][0].label\n",
        "            edge_index = data_all[0][0].graph['edge_index']\n",
        "\n",
        "        if args.model == \"GCN\" or args.model == \"GCNSLAPS\":\n",
        "            save_mem = False\n",
        "            model = GCN(nfeat=feat.shape[1], nhid=args.hidden, dropout=args.dropout, nlayers=args.nlayers,\n",
        "                        weight_decay=args.weight_decay, with_bn=True, lr=args.lr, save_mem=save_mem,\n",
        "                        nclass=max(labels).item()+1, device=device, args=args).to(device)\n",
        "\n",
        "        elif args.model == \"GAT\":\n",
        "            model = GAT(nfeat=feat.shape[1], nhid=32, heads=4, lr=args.lr, nlayers=args.nlayers,\n",
        "                  nclass=labels.max().item() + 1, with_bn=True, weight_decay=args.weight_decay,\n",
        "                  dropout=0.0, device=device, args=args).to(device)\n",
        "\n",
        "        elif args.model == \"SAGE\":\n",
        "            if args.dataset == \"fb100\":\n",
        "                model = SAGE2(feat.shape[1], 32, max(labels).item()+1, num_layers=args.nlayers,\n",
        "                        dropout=0.0, lr=0.01, weight_decay=args.weight_decay,\n",
        "                        device=device, args=args, with_bn=args.with_bn).to(device)\n",
        "            else:\n",
        "                model = SAGE(feat.shape[1], 32, max(labels).item()+1, num_layers=args.nlayers,\n",
        "                        dropout=0.0, lr=0.01, weight_decay=args.weight_decay, device=device,\n",
        "                        args=args, with_bn=args.with_bn).to(device)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        if self.args.verbose:\n",
        "          if verbose: print(model)\n",
        "\n",
        "        import os.path as osp\n",
        "        if args.ood:\n",
        "            filename = f'{DATADIR}/pretrained_models/{args.dataset}_{args.model}_s{args.seed}.pt'\n",
        "        else:\n",
        "            filename = f'saved_no_ood/{args.dataset}_{args.model}_s{args.seed}.pt'\n",
        "        if args.debug and osp.exists(filename):\n",
        "            model.load_state_dict(torch.load(filename, map_location=self.device))\n",
        "        else:\n",
        "            # do pre-training again every-time (fast)\n",
        "            train_iters = 500 if args.dataset == 'ogb-arxiv' else 200\n",
        "            model.fit_inductive(data_all, train_iters=train_iters, patience=500, verbose=args.verbose)\n",
        "            if args.debug:\n",
        "              if self.args.verbose:\n",
        "                print(\"save model state_dict to\", filename)\n",
        "              torch.save(model.state_dict(), filename)\n",
        "        if args.model == \"GCNSLAPS\":\n",
        "            assert args.debug > 0\n",
        "            model.setup_dae(feat.shape[1], nhid=args.hidden, nclass=feat.shape[1])\n",
        "            model.train_dae(feat, edge_index, None)\n",
        "\n",
        "        if verbose: self.evaluate(model)\n",
        "        return model\n",
        "\n",
        "    def evaluate_single(self, model, output, labels, test_data, verbose=True):\n",
        "        eval_func = model.eval_func\n",
        "        if self.args.dataset in ['ogb-arxiv']:\n",
        "            acc_test = eval_func(labels[test_data.test_mask], output[test_data.test_mask])\n",
        "        elif self.args.dataset in ['cora', 'amazon-photo', 'twitch-e', 'fb100']:\n",
        "            acc_test = eval_func(labels, output)\n",
        "        elif self.args.dataset in ['elliptic']:\n",
        "            acc_test = eval_func(labels[test_data.mask], output[test_data.mask])\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        if self.args.verbose:\n",
        "          if verbose:\n",
        "            print('Test:', acc_test)\n",
        "        return acc_test\n",
        "\n",
        "    def evaluate(self, model):\n",
        "        model.eval()\n",
        "        accs = []\n",
        "        y_te, out_te = [], []\n",
        "        y_te_all, out_te_all = [], []\n",
        "        for ii, test_data in enumerate(self.data_all[2]):\n",
        "            x, edge_index = test_data.graph['node_feat'], test_data.graph['edge_index']\n",
        "            x, edge_index = x.to(self.device), edge_index.to(self.device)\n",
        "            output = model.predict(x, edge_index)\n",
        "\n",
        "            labels = test_data.label.to(self.device) #.squeeze()\n",
        "            eval_func = model.eval_func\n",
        "            if self.args.dataset in ['ogb-arxiv']:\n",
        "                acc_test = eval_func(labels[test_data.test_mask], output[test_data.test_mask])\n",
        "                accs.append(acc_test)\n",
        "                y_te_all.append(labels[test_data.test_mask])\n",
        "                out_te_all.append(output[test_data.test_mask])\n",
        "            elif self.args.dataset in ['cora', 'amazon-photo', 'twitch-e', 'fb100']:\n",
        "                acc_test = eval_func(labels, output)\n",
        "                accs.append(acc_test)\n",
        "                y_te_all.append(labels)\n",
        "                out_te_all.append(output)\n",
        "            elif self.args.dataset in ['elliptic']:\n",
        "                acc_test = eval_func(labels[test_data.mask], output[test_data.mask])\n",
        "                y_te.append(labels[test_data.mask])\n",
        "                out_te.append(output[test_data.mask])\n",
        "                y_te_all.append(labels[test_data.mask])\n",
        "                out_te_all.append(output[test_data.mask])\n",
        "                if ii % 4 == 0 or ii == len(self.data_all[2]) - 1:\n",
        "                    acc_te = eval_func(torch.cat(y_te, dim=0), torch.cat(out_te, dim=0))\n",
        "                    accs += [float(f'{acc_te:.2f}')]\n",
        "                    y_te, out_te = [], []\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "        if self.args.verbose:\n",
        "          print('Test accs:', accs)\n",
        "        acc_te = eval_func(torch.cat(y_te_all, dim=0), torch.cat(out_te_all, dim=0))\n",
        "        if self.args.verbose:\n",
        "          print(f'flatten test: {acc_te}')\n",
        "\n",
        "    def get_perf(self, output, labels, mask):\n",
        "        loss = F.nll_loss(output[mask], labels[mask])\n",
        "        acc = utils.accuracy(output[mask], labels[mask])\n",
        "        print(\"loss= {:.4f}\".format(loss.item()),\n",
        "              \"accuracy= {:.4f}\".format(acc.item()))\n",
        "        return loss.item(), acc.item()\n",
        "\n",
        "@torch.jit.script\n",
        "def softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Entropy of softmax distribution from **logits**.\"\"\"\n",
        "    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n",
        "\n",
        "@torch.jit.script\n",
        "def entropy(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Entropy of softmax distribution from **log_softmax**.\"\"\"\n",
        "    return -(x * torch.log(x+1e-15)).sum(1)\n",
        "\n",
        "def compare_models(model1, model2):\n",
        "    for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
        "        if p1.data.ne(p2.data).sum() > 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def sim(t1, t2):\n",
        "    # cosine similarity\n",
        "    t1 = t1 / (t1.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    t2 = t2 / (t2.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    return (t1 * t2).sum(1)\n",
        "\n",
        "def inner(t1, t2):\n",
        "    t1 = t1 / (t1.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    t2 = t2 / (t2.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    return (1-(t1 * t2).sum(1)).mean()\n",
        "\n",
        "def inner_margin(t1, t2, margin):\n",
        "    t1 = t1 / (t1.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    t2 = t2 / (t2.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    return F.relu(1-(t1 * t2).sum(1)-margin).mean()\n",
        "\n",
        "def diff(t1, t2):\n",
        "    t1 = t1 / (t1.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    t2 = t2 / (t2.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    return 0.5*((t1-t2)**2).sum(1).mean()\n",
        "\n",
        "\n",
        "\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        # print(self.base_optimizer, self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbvagDZjfqMn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSO29z_L0fM2"
      },
      "source": [
        "#Edge Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfGLCVbshFmy"
      },
      "outputs": [],
      "source": [
        "\"\"\"learn edge indices\"\"\"\n",
        "import numpy as np\n",
        "# from models import *\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "# import deeprobust.graph.utils as utils\n",
        "from torch.nn.parameter import Parameter\n",
        "from tqdm import tqdm\n",
        "# from gtransform_feat import FeatAgent\n",
        "import torch_sparse\n",
        "from torch_sparse import coalesce\n",
        "import math\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix, dropout_adj, is_undirected, to_undirected\n",
        "\n",
        "\n",
        "class EdgeAgent(FeatAgent):\n",
        "\n",
        "    def __init__(self, data_all, args):\n",
        "        self.device = 'cuda'\n",
        "        self.args = args\n",
        "        self.data_all = data_all\n",
        "        # pre-train model again every time\n",
        "        self.model = self.pretrain_model()\n",
        "\n",
        "    def setup_params(self, data):\n",
        "        args = self.args\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        nnodes = data.graph['node_feat'].shape[0]\n",
        "        d = data.graph['node_feat'].shape[1]\n",
        "\n",
        "        self.n, self.d = nnodes, nnodes\n",
        "\n",
        "        self.make_undirected = True\n",
        "        self.max_final_samples = 20\n",
        "        self.search_space_size = 10_000_000\n",
        "        self.eps = 1e-7\n",
        "\n",
        "        self.modified_edge_index: torch.Tensor = None\n",
        "        self.perturbed_edge_weight: torch.Tensor = None\n",
        "        if self.make_undirected:\n",
        "            self.n_possible_edges = self.n * (self.n - 1) // 2\n",
        "        else:\n",
        "            self.n_possible_edges = self.n ** 2  # We filter self-loops later\n",
        "\n",
        "        lr_factor = args.lr_adj\n",
        "        self.lr_factor = lr_factor * max(math.log2(self.n_possible_edges / self.search_space_size), 1.)\n",
        "        self.epochs_resampling = self.args.epochs\n",
        "        self.with_early_stopping = True\n",
        "        self.do_synchronize = True\n",
        "\n",
        "    # re-write\n",
        "    def learn_graph(self, data):\n",
        "        self.setup_params(data)\n",
        "        args = self.args\n",
        "        model = self.model\n",
        "        model.eval() # should set to eval\n",
        "\n",
        "        feat, labels = data.graph['node_feat'].to(self.device), data.label.to(self.device)#.squeeze()\n",
        "        self.edge_index = data.graph['edge_index'].to(self.device)\n",
        "        self.edge_weight = torch.ones(self.edge_index.shape[1]).to(self.device)\n",
        "        self.feat = feat\n",
        "\n",
        "        n_perturbations = int(args.ratio * self.edge_index.shape[1] //2)\n",
        "        print('n_perturbations:', n_perturbations)\n",
        "        self.sample_random_block(n_perturbations)\n",
        "\n",
        "        self.perturbed_edge_weight.requires_grad = True\n",
        "        self.optimizer_adj = torch.optim.Adam([self.perturbed_edge_weight], lr=args.lr_adj)\n",
        "        for it in tqdm(range(args.epochs)):\n",
        "            self.perturbed_edge_weight.requires_grad = True\n",
        "            edge_index, edge_weight  = self.get_modified_adj()\n",
        "            if torch.cuda.is_available() and self.do_synchronize:\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            loss = self.test_time_loss(model, feat, edge_index, edge_weight)\n",
        "            gradient = grad_with_checkpoint(loss, self.perturbed_edge_weight)[0]\n",
        "\n",
        "            if torch.cuda.is_available() and self.do_synchronize:\n",
        "                torch.cuda.empty_cache()\n",
        "                torch.cuda.synchronize()\n",
        "            if it == 0:\n",
        "                print(f'Epoch {it}: {loss}')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.update_edge_weights(n_perturbations, it, gradient)\n",
        "                self.perturbed_edge_weight = self.project(\n",
        "                    n_perturbations, self.perturbed_edge_weight, self.eps)\n",
        "                del edge_index, edge_weight #, logits\n",
        "\n",
        "                if not args.existing_space:\n",
        "                    if it < self.epochs_resampling - 1:\n",
        "                        self.resample_random_block(n_perturbations)\n",
        "            if it < self.epochs_resampling - 1:\n",
        "                self.perturbed_edge_weight.requires_grad = True\n",
        "                self.optimizer_adj = torch.optim.Adam([self.perturbed_edge_weight], lr=args.lr_adj)\n",
        "\n",
        "        print(f'Epoch {it}: {loss}')\n",
        "        edge_index, edge_weight = self.sample_final_edges(n_perturbations, data)\n",
        "        loss = self.test_time_loss(self.model, feat, edge_index, edge_weight)\n",
        "        print('final loss:', loss.item())\n",
        "\n",
        "        output = model.predict(feat, edge_index, edge_weight)\n",
        "        print('Test:')\n",
        "\n",
        "        if args.dataset == 'elliptic':\n",
        "            return self.evaluate_single(model, output, labels, data), output[data.mask], labels[data.mask]\n",
        "        else:\n",
        "            return self.evaluate_single(model, output, labels, data), output, labels\n",
        "\n",
        "    # re-write\n",
        "    def augment(self, strategy='dropedge', p=0.5, edge_index=None, edge_weight=None):\n",
        "        model = self.model\n",
        "        feat = self.feat\n",
        "        if strategy == 'shuffle':\n",
        "            idx = np.random.permutation(feat.shape[0])\n",
        "            shuf_fts = feat[idx, :]\n",
        "            output = model.get_embed(shuf_fts, edge_index, edge_weight)\n",
        "        if strategy == \"dropedge\":\n",
        "            edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"dropfeat\":\n",
        "            feat = F.dropout(feat, p=p)\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"featnoise\":\n",
        "            mean, std = 0, p\n",
        "            noise = torch.randn(feat.size()) * std + mean\n",
        "            feat = feat + noise.to(feat.device)\n",
        "            output = model.get_embed(feat, edge_index)\n",
        "        return output\n",
        "\n",
        "    def sample_random_block(self, n_perturbations):\n",
        "        if self.args.existing_space:\n",
        "            edge_index = self.edge_index.clone()\n",
        "            edge_index = edge_index[:, edge_index[0] < edge_index[1]]\n",
        "            row, col = edge_index[0], edge_index[1]\n",
        "            edge_index_id = (2*self.n - row-1)*row//2 + col - row -1 # // is important to get the correct result\n",
        "            edge_index_id = edge_index_id.long()\n",
        "            self.current_search_space = edge_index_id\n",
        "            self.modified_edge_index = linear_to_triu_idx(self.n, self.current_search_space)\n",
        "            self.perturbed_edge_weight = torch.full_like(\n",
        "                self.current_search_space, self.eps, dtype=torch.float32, requires_grad=True\n",
        "            )\n",
        "\n",
        "            return\n",
        "        for _ in range(self.max_final_samples):\n",
        "\n",
        "            self.current_search_space = torch.randint(\n",
        "                self.n_possible_edges, (self.search_space_size,), device=self.device)\n",
        "            self.current_search_space = torch.unique(self.current_search_space, sorted=True)\n",
        "            if self.make_undirected:\n",
        "                self.modified_edge_index = linear_to_triu_idx(self.n, self.current_search_space)\n",
        "            else:\n",
        "                self.modified_edge_index = linear_to_full_idx(self.n, self.current_search_space)\n",
        "                is_not_self_loop = self.modified_edge_index[0] != self.modified_edge_index[1]\n",
        "                self.current_search_space = self.current_search_space[is_not_self_loop]\n",
        "                self.modified_edge_index = self.modified_edge_index[:, is_not_self_loop]\n",
        "\n",
        "            self.perturbed_edge_weight = torch.full_like(\n",
        "                self.current_search_space, self.eps, dtype=torch.float32, requires_grad=True\n",
        "            )\n",
        "            if self.current_search_space.size(0) >= n_perturbations:\n",
        "                return\n",
        "        raise RuntimeError('Sampling random block was not successfull. Please decrease `n_perturbations`.')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample_final_edges(self, n_perturbations, data):\n",
        "        best_loss = float('Inf')\n",
        "        perturbed_edge_weight = self.perturbed_edge_weight.detach()\n",
        "        perturbed_edge_weight[perturbed_edge_weight <= self.eps] = 0\n",
        "        feat, labels = data.graph['node_feat'].to(self.device), data.label.to(self.device).squeeze()\n",
        "        for i in range(self.max_final_samples):\n",
        "            if best_loss == float('Inf'):\n",
        "                # In first iteration employ top k heuristic instead of sampling\n",
        "                sampled_edges = torch.zeros_like(perturbed_edge_weight)\n",
        "                sampled_edges[torch.topk(perturbed_edge_weight, n_perturbations).indices] = 1\n",
        "            else:\n",
        "                sampled_edges = torch.bernoulli(perturbed_edge_weight).float()\n",
        "\n",
        "            if sampled_edges.sum() > n_perturbations:\n",
        "                n_samples = sampled_edges.sum()\n",
        "                if self.args.debug ==2:\n",
        "                    print(f'{i}-th sampling: too many samples {n_samples}')\n",
        "                continue\n",
        "            self.perturbed_edge_weight = sampled_edges\n",
        "\n",
        "            edge_index, edge_weight = self.get_modified_adj()\n",
        "            with torch.no_grad():\n",
        "                # output = self.model.forward(feat, edge_index, edge_weight)\n",
        "                loss = self.test_time_loss(self.model, feat, edge_index, edge_weight, mode='eval')\n",
        "            # Save best sample\n",
        "            if best_loss > loss:\n",
        "                best_loss = loss\n",
        "                print('best_loss:', best_loss)\n",
        "                best_edges = self.perturbed_edge_weight.clone().cpu()\n",
        "\n",
        "        # Recover best sample\n",
        "        self.perturbed_edge_weight.data.copy_(best_edges.to(self.device))\n",
        "        edge_index, edge_weight = self.get_modified_adj()\n",
        "        edge_mask = edge_weight == 1\n",
        "\n",
        "        allowed_perturbations = 2 * n_perturbations if self.make_undirected else n_perturbations\n",
        "        edges_after_attack = edge_mask.sum()\n",
        "        clean_edges = self.edge_index.shape[1]\n",
        "        assert (edges_after_attack >= clean_edges - allowed_perturbations\n",
        "                and edges_after_attack <= clean_edges + allowed_perturbations), \\\n",
        "            f'{edges_after_attack} out of range with {clean_edges} clean edges and {n_perturbations} pertutbations'\n",
        "        return edge_index[:, edge_mask], edge_weight[edge_mask]\n",
        "\n",
        "    def resample_random_block(self, n_perturbations: int):\n",
        "        self.keep_heuristic = 'WeightOnly'\n",
        "        if self.keep_heuristic == 'WeightOnly':\n",
        "            sorted_idx = torch.argsort(self.perturbed_edge_weight)\n",
        "            idx_keep = (self.perturbed_edge_weight <= self.eps).sum().long()\n",
        "            # Keep at most half of the block (i.e. resample low weights)\n",
        "            if idx_keep < sorted_idx.size(0) // 2:\n",
        "                idx_keep = sorted_idx.size(0) // 2\n",
        "        else:\n",
        "            raise NotImplementedError('Only keep_heuristic=`WeightOnly` supported')\n",
        "\n",
        "        sorted_idx = sorted_idx[idx_keep:]\n",
        "        self.current_search_space = self.current_search_space[sorted_idx]\n",
        "        self.modified_edge_index = self.modified_edge_index[:, sorted_idx]\n",
        "        self.perturbed_edge_weight = self.perturbed_edge_weight[sorted_idx]\n",
        "\n",
        "        # Sample until enough edges were drawn\n",
        "        for i in range(self.max_final_samples):\n",
        "            n_edges_resample = self.search_space_size - self.current_search_space.size(0)\n",
        "            lin_index = torch.randint(self.n_possible_edges, (n_edges_resample,), device=self.device)\n",
        "\n",
        "            self.current_search_space, unique_idx = torch.unique(\n",
        "                torch.cat((self.current_search_space, lin_index)),\n",
        "                sorted=True,\n",
        "                return_inverse=True\n",
        "            )\n",
        "\n",
        "            if self.make_undirected:\n",
        "                self.modified_edge_index = linear_to_triu_idx(self.n, self.current_search_space)\n",
        "            else:\n",
        "                self.modified_edge_index = linear_to_full_idx(self.n, self.current_search_space)\n",
        "\n",
        "            # Merge existing weights with new edge weights\n",
        "            perturbed_edge_weight_old = self.perturbed_edge_weight.clone()\n",
        "            self.perturbed_edge_weight = torch.full_like(self.current_search_space, self.eps, dtype=torch.float32)\n",
        "            self.perturbed_edge_weight[\n",
        "                unique_idx[:perturbed_edge_weight_old.size(0)]\n",
        "            ] = perturbed_edge_weight_old\n",
        "\n",
        "            if not self.make_undirected:\n",
        "                is_not_self_loop = self.modified_edge_index[0] != self.modified_edge_index[1]\n",
        "                self.current_search_space = self.current_search_space[is_not_self_loop]\n",
        "                self.modified_edge_index = self.modified_edge_index[:, is_not_self_loop]\n",
        "                self.perturbed_edge_weight = self.perturbed_edge_weight[is_not_self_loop]\n",
        "\n",
        "            if self.current_search_space.size(0) > n_perturbations:\n",
        "                return\n",
        "        raise RuntimeError('Sampling random block was not successfull. Please decrease `n_perturbations`.')\n",
        "\n",
        "    def project(self, n_perturbations, values, eps, inplace=False):\n",
        "        if not inplace:\n",
        "            values = values.clone()\n",
        "\n",
        "        if torch.clamp(values, 0, 1).sum() > n_perturbations:\n",
        "            left = (values - 1).min()\n",
        "            right = values.max()\n",
        "            miu = bisection(values, left, right, n_perturbations)\n",
        "            values.data.copy_(torch.clamp(\n",
        "                values - miu, min=eps, max=1 - eps\n",
        "            ))\n",
        "        else:\n",
        "            values.data.copy_(torch.clamp(\n",
        "                values, min=eps, max=1 - eps\n",
        "            ))\n",
        "        return values\n",
        "\n",
        "    def get_modified_adj(self):\n",
        "        if self.make_undirected:\n",
        "            modified_edge_index, modified_edge_weight = to_symmetric(\n",
        "                self.modified_edge_index, self.perturbed_edge_weight, self.n\n",
        "            )\n",
        "        else:\n",
        "            modified_edge_index, modified_edge_weight = self.modified_edge_index, self.perturbed_edge_weight\n",
        "        edge_index = torch.cat((self.edge_index.to(self.device), modified_edge_index), dim=-1)\n",
        "        edge_weight = torch.cat((self.edge_weight.to(self.device), modified_edge_weight))\n",
        "\n",
        "        edge_index, edge_weight = torch_sparse.coalesce(edge_index, edge_weight, m=self.n, n=self.n, op='sum')\n",
        "\n",
        "        # Allow removal of edges\n",
        "        edge_weight[edge_weight > 1] = 2 - edge_weight[edge_weight > 1]\n",
        "        return edge_index, edge_weight\n",
        "\n",
        "    def _update_edge_weights(self, n_perturbations, epoch, gradient):\n",
        "        lr_factor = n_perturbations / self.n / 2 * self.lr_factor\n",
        "        lr = lr_factor / np.sqrt(max(0, epoch - self.epochs_resampling) + 1)\n",
        "        self.perturbed_edge_weight.data.add_(-lr * gradient)\n",
        "        # We require for technical reasons that all edges in the block have at least a small positive value\n",
        "        self.perturbed_edge_weight.data[self.perturbed_edge_weight < self.eps] = self.eps\n",
        "\n",
        "    def update_edge_weights(self, n_perturbations, epoch, gradient):\n",
        "        self.optimizer_adj.zero_grad()\n",
        "        self.perturbed_edge_weight.grad = gradient\n",
        "        self.optimizer_adj.step()\n",
        "        self.perturbed_edge_weight.data[self.perturbed_edge_weight < self.eps] = self.eps\n",
        "\n",
        "@torch.jit.script\n",
        "def softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Entropy of softmax distribution from **logits**.\"\"\"\n",
        "    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n",
        "\n",
        "@torch.jit.script\n",
        "def entropy(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Entropy of softmax distribution from **log_softmax**.\"\"\"\n",
        "    return -(torch.exp(x) * x).sum(1)\n",
        "\n",
        "\n",
        "def to_symmetric(edge_index, edge_weight, n, op='mean'):\n",
        "    symmetric_edge_index = torch.cat(\n",
        "        (edge_index, edge_index.flip(0)), dim=-1\n",
        "    )\n",
        "\n",
        "    symmetric_edge_weight = edge_weight.repeat(2)\n",
        "\n",
        "    symmetric_edge_index, symmetric_edge_weight = coalesce(\n",
        "        symmetric_edge_index,\n",
        "        symmetric_edge_weight,\n",
        "        m=n,\n",
        "        n=n,\n",
        "        op=op\n",
        "    )\n",
        "    return symmetric_edge_index, symmetric_edge_weight\n",
        "\n",
        "def linear_to_triu_idx(n: int, lin_idx: torch.Tensor) -> torch.Tensor:\n",
        "    row_idx = (\n",
        "        n\n",
        "        - 2\n",
        "        - torch.floor(torch.sqrt(-8 * lin_idx.double() + 4 * n * (n - 1) - 7) / 2.0 - 0.5)\n",
        "    ).long()\n",
        "    col_idx = (\n",
        "        lin_idx\n",
        "        + row_idx\n",
        "        + 1 - n * (n - 1) // 2\n",
        "        + (n - row_idx) * ((n - row_idx) - 1) // 2\n",
        "    )\n",
        "    return torch.stack((row_idx, col_idx))\n",
        "\n",
        "\n",
        "def grad_with_checkpoint(outputs, inputs):\n",
        "    inputs = (inputs,) if isinstance(inputs, torch.Tensor) else tuple(inputs)\n",
        "    for input in inputs:\n",
        "        if not input.is_leaf:\n",
        "            input.retain_grad()\n",
        "    torch.autograd.backward(outputs)\n",
        "\n",
        "    grad_outputs = []\n",
        "    for input in inputs:\n",
        "        grad_outputs.append(input.grad.clone())\n",
        "        input.grad.zero_()\n",
        "    return grad_outputs\n",
        "\n",
        "def bisection(edge_weights, a, b, n_perturbations, epsilon=1e-5, iter_max=1e5):\n",
        "    def func(x):\n",
        "        return torch.clamp(edge_weights - x, 0, 1).sum() - n_perturbations\n",
        "\n",
        "    miu = a\n",
        "    for i in range(int(iter_max)):\n",
        "        miu = (a + b) / 2\n",
        "        # Check if middle point is root\n",
        "        if (func(miu) == 0.0):\n",
        "            break\n",
        "        # Decide the side to repeat the steps\n",
        "        if (func(miu) * func(a) < 0):\n",
        "            b = miu\n",
        "        else:\n",
        "            a = miu\n",
        "        if ((b - a) <= epsilon):\n",
        "            break\n",
        "    return miu\n",
        "\n",
        "def homophily(adj, labels):\n",
        "    edge_index = adj.nonzero()\n",
        "    homo = (labels[edge_index[0]] == labels[edge_index[1]])\n",
        "    return np.mean(homo.numpy())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-tkRBPa0ifw"
      },
      "source": [
        "#Graph Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aJMq8s4hMLW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# from models import *\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "# import deeprobust.graph.utils as utils\n",
        "from torch.nn.parameter import Parameter\n",
        "from tqdm import tqdm\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from copy import deepcopy\n",
        "# from utils import reset_args\n",
        "# from gtransform_adj import EdgeAgent\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix, dropout_adj, is_undirected, to_undirected, dropout_edge\n",
        "# from gtransform_adj import *\n",
        "\n",
        "class GraphAgent(EdgeAgent):\n",
        "\n",
        "    def __init__(self, data_all, args):\n",
        "        self.device = 'cuda'\n",
        "        self.args = args\n",
        "        self.data_all = data_all\n",
        "        # pre-train model again every time\n",
        "        self.model = self.pretrain_model()\n",
        "\n",
        "    # re-write\n",
        "    def learn_graph(self, data):\n",
        "        print('====learning on this graph===')\n",
        "        args = self.args\n",
        "        self.setup_params(data)\n",
        "        args = self.args\n",
        "        model = self.model\n",
        "        model.eval() # should set to eval\n",
        "\n",
        "        self.max_final_samples = 5\n",
        "\n",
        "        # from utils import get_gpu_memory_map\n",
        "        mem_st = get_gpu_memory_map()\n",
        "        args = self.args\n",
        "        self.data = data\n",
        "        nnodes = data.graph['node_feat'].shape[0]\n",
        "        d = data.graph['node_feat'].shape[1]\n",
        "\n",
        "        delta_feat = Parameter(torch.FloatTensor(nnodes, d).to(self.device))\n",
        "        self.delta_feat = delta_feat\n",
        "        delta_feat.data.fill_(1e-7)\n",
        "        self.optimizer_feat = torch.optim.Adam([delta_feat], lr=args.lr_feat)\n",
        "\n",
        "        model = self.model\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        model.eval() # should set to eval\n",
        "\n",
        "        feat, labels = data.graph['node_feat'].to(self.device), data.label.to(self.device)#.squeeze()\n",
        "        edge_index = data.graph['edge_index'].to(self.device)\n",
        "        self.edge_index, self.feat, self.labels = edge_index, feat, labels\n",
        "        self.edge_weight = torch.ones(self.edge_index.shape[1]).to(self.device)\n",
        "\n",
        "        n_perturbations = int(args.ratio * self.edge_index.shape[1] //2)\n",
        "        print('n_perturbations:', n_perturbations)\n",
        "        self.sample_random_block(n_perturbations)\n",
        "\n",
        "        self.perturbed_edge_weight.requires_grad = True\n",
        "        self.optimizer_adj = torch.optim.Adam([self.perturbed_edge_weight], lr=args.lr_adj)\n",
        "        edge_index, edge_weight = edge_index, None\n",
        "\n",
        "        print(\"lf, la, ep, ep/(lf+la)\")\n",
        "        print(args.loop_feat, args.loop_adj, args.epochs, args.epochs//(args.loop_feat+args.loop_adj))\n",
        "        for it in tqdm(range(args.epochs//(args.loop_feat+args.loop_adj))):\n",
        "            print(\"start loop feat\")\n",
        "            for loop_feat in range(args.loop_feat):\n",
        "                self.optimizer_feat.zero_grad()\n",
        "                loss = self.test_time_loss(model, feat+delta_feat, edge_index, edge_weight)\n",
        "                loss.backward()\n",
        "                print(loss)\n",
        "\n",
        "                if loop_feat == 0:\n",
        "                    print(f'Epoch {it}, Loop Feat {loop_feat}: {loss.item()}')\n",
        "\n",
        "                self.optimizer_feat.step()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    # torch.cuda.synchronize()\n",
        "                # if args.debug==2 or args.debug==3:\n",
        "                #     output = model.predict(feat+delta_feat, edge_index, edge_weight)\n",
        "                #     print('Debug Test:', self.evaluate_single(model, output, labels, data, verbose=0))\n",
        "\n",
        "                print(\"step\")\n",
        "            print(\"start loop adj\")\n",
        "            new_feat = (feat+delta_feat).detach()\n",
        "            for loop_adj in range(args.loop_adj):\n",
        "                self.perturbed_edge_weight.requires_grad = True\n",
        "                edge_index, edge_weight  = self.get_modified_adj()\n",
        "                if torch.cuda.is_available() and self.do_synchronize:\n",
        "                    torch.cuda.empty_cache()\n",
        "                    torch.cuda.synchronize()\n",
        "\n",
        "                loss = self.test_time_loss(model, new_feat, edge_index, edge_weight)\n",
        "\n",
        "                gradient = grad_with_checkpoint(loss, self.perturbed_edge_weight)[0]\n",
        "                # if not args.existing_space:\n",
        "                #     if torch.cuda.is_available() and self.do_synchronize:\n",
        "                #         torch.cuda.empty_cache()\n",
        "                #         torch.cuda.synchronize()\n",
        "\n",
        "                if loop_adj == 0:\n",
        "                    print(f'Epoch {it}, Loop Adj {loop_adj}: {loss.item()}')\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    self.update_edge_weights(n_perturbations, it, gradient)\n",
        "                    self.perturbed_edge_weight = self.project(\n",
        "                        n_perturbations, self.perturbed_edge_weight, self.eps)\n",
        "                    del edge_index, edge_weight #, logits\n",
        "                    if not args.existing_space:\n",
        "                        if it < self.epochs_resampling - 1:\n",
        "                            self.resample_random_block(n_perturbations)\n",
        "                if it < self.epochs_resampling - 1:\n",
        "                    self.perturbed_edge_weight.requires_grad = True\n",
        "                    self.optimizer_adj = torch.optim.Adam([self.perturbed_edge_weight], lr=args.lr_adj)\n",
        "\n",
        "            # edge_index, edge_weight = self.sample_final_edges(n_perturbations, data)\n",
        "            if args.loop_adj != 0:\n",
        "                edge_index, edge_weight  = self.get_modified_adj()\n",
        "                edge_weight = edge_weight.detach()\n",
        "\n",
        "        print(f'Epoch {it+1}: {loss}')\n",
        "        gpu_mem = get_gpu_memory_map()\n",
        "        print(f'Mem used: {int(gpu_mem[args.gpu_id])-int(mem_st[args.gpu_id])}MB')\n",
        "\n",
        "        if args.loop_adj != 0:\n",
        "            edge_index, edge_weight = self.sample_final_edges(n_perturbations, data)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss = self.test_time_loss(model, feat+delta_feat, edge_index, edge_weight)\n",
        "        print('final loss:', loss.item())\n",
        "        output = model.predict(feat+delta_feat, edge_index, edge_weight)\n",
        "        print('Test:')\n",
        "\n",
        "        if args.dataset == 'elliptic':\n",
        "            return self.evaluate_single(model, output, labels, data), output[data.mask], labels[data.mask]\n",
        "        else:\n",
        "            return self.evaluate_single(model, output, labels, data), output, labels\n",
        "\n",
        "    # re-write\n",
        "    def augment(self, strategy='dropedge', p=0.5, edge_index=None, edge_weight=None):\n",
        "        model = self.model\n",
        "        if hasattr(self, 'delta_feat'):\n",
        "            delta_feat = self.delta_feat\n",
        "            feat = self.feat + delta_feat\n",
        "        else:\n",
        "            feat = self.feat\n",
        "        if strategy == 'shuffle':\n",
        "            idx = np.random.permutation(feat.shape[0])\n",
        "            shuf_fts = feat[idx, :]\n",
        "            output = model.get_embed(shuf_fts, edge_index, edge_weight)\n",
        "        if strategy == \"dropedge\":\n",
        "            # edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            edge_index, edge_mask = dropout_edge(edge_index, p=p)\n",
        "            edge_weight = edge_weight[edge_mask] if edge_weight != None else None\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"dropnode\":\n",
        "            feat = self.feat + self.delta_feat\n",
        "            mask = torch.cuda.FloatTensor(len(feat)).uniform_() > p\n",
        "            feat = feat * mask.view(-1, 1)\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"rwsample\":\n",
        "            import augmentor as A\n",
        "            if self.args.dataset in ['twitch-e', 'elliptic']:\n",
        "                walk_length = 1\n",
        "            else:\n",
        "                walk_length = 10\n",
        "            aug = A.RWSampling(num_seeds=1000, walk_length=walk_length)\n",
        "            x = self.feat + self.delta_feat\n",
        "            x2, edge_index2, edge_weight2 = aug(x, edge_index, edge_weight)\n",
        "            output = model.get_embed(x2, edge_index2, edge_weight2)\n",
        "\n",
        "        if strategy == \"dropmix\":\n",
        "            feat = self.feat + self.delta_feat\n",
        "            mask = torch.cuda.FloatTensor(len(feat)).uniform_() > p\n",
        "            feat = feat * mask.view(-1, 1)\n",
        "            # edge_index, edge_weight = dropout_adj(edge_index, edge_weight, p=p)\n",
        "            edge_index, edge_mask = dropout_edge(edge_index, p=p)\n",
        "            edge_weight = edge_weight[edge_mask] if edge_weight != None else None\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "\n",
        "        if strategy == \"dropfeat\":\n",
        "            feat = F.dropout(self.feat, p=p) + self.delta_feat\n",
        "            output = model.get_embed(feat, edge_index, edge_weight)\n",
        "        if strategy == \"featnoise\":\n",
        "            mean, std = 0, p\n",
        "            noise = torch.randn(feat.size()) * std + mean\n",
        "            feat = feat + noise.to(feat.device)\n",
        "            output = model.get_embed(feat, edge_index)\n",
        "        return output\n",
        "\n",
        "def inner(t1, t2):\n",
        "    t1 = t1 / (t1.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    t2 = t2 / (t2.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    return (1-(t1 * t2).sum(1)).mean()\n",
        "\n",
        "def diff(t1, t2):\n",
        "    t1 = t1 / (t1.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    t2 = t2 / (t2.norm(dim=1).view(-1,1) + 1e-15)\n",
        "    return 0.5*((t1-t2)**2).sum(1).mean()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7a-Y3q40xFI"
      },
      "source": [
        "#main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxzUVxrgKzl-"
      },
      "source": [
        "## default argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1aXJ330Ku1E"
      },
      "outputs": [],
      "source": [
        "class ARGS():\n",
        "  def __init__(self):\n",
        "    # default\n",
        "\n",
        "    self.gpu_id = 0\n",
        "    self.dataset = \"cora\" # cora # amazon-photo # elliptic # twitch-e # fb100\n",
        "    self.epochs=50\n",
        "    self.hidden=32\n",
        "    self.weight_decay=5e-4\n",
        "    self.normalize_features=True\n",
        "    self.seed=0\n",
        "    self.lr=0.01\n",
        "    self.lr_feat=0.001\n",
        "    self.nlayers=5\n",
        "    self.model=\"GCN\"\n",
        "    self.loss=\"LC\"\n",
        "    self.debug=1\n",
        "    self.ood=1\n",
        "    self.with_bn=1\n",
        "    self.lr_adj=0.1\n",
        "    self.ratio=0.1\n",
        "    self.margin=-1\n",
        "    self.existing_space=1\n",
        "    self.loop_adj=1\n",
        "    self.loop_feat=4\n",
        "    self.test_val=0\n",
        "    self.tune=0\n",
        "    self.finetune=0 # if want to fine-tune model to\n",
        "    self.tent=0 # if want to use TENT as the TTA model\n",
        "    self.strategy=\"dropedge\"\n",
        "\n",
        "    self.bn_momentum=0.1 # by default\n",
        "    self.use_learned_stats=1 # if want to\n",
        "    self.conf_filter=0.0\n",
        "    self.ent_filter=1.0\n",
        "    self.ep_tta=1\n",
        "    self.lr_tta=0.001\n",
        "    self.sam=False\n",
        "    self.train_all=False\n",
        "    self.not_reset=False\n",
        "    self.verbose=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVsG-Bidt9E"
      },
      "source": [
        "# Eval all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16oA8eGyjEu3"
      },
      "outputs": [],
      "source": [
        "dataset_ls = ['amazon-photo', 'cora', 'elliptic', 'twitch-e']\n",
        "# dataset_ls = ['amazon-photo', 'cora', 'elliptic', 'twitch-e']\n",
        "seeds = list(range(10)) # average the results on 10 different seeds\n",
        "model_ls = [\"GCN\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRY1Uq_zZ7pp"
      },
      "outputs": [],
      "source": [
        "args_ = {}\n",
        "\n",
        "args1 = ARGS()\n",
        "args1.ent_filter=0.1\n",
        "args1.ep_tta=1\n",
        "args1.lr_tta=0.00001\n",
        "args1.train_all=True\n",
        "args1.not_reset=False\n",
        "args1.dropout_inference=4\n",
        "args1.dropout_rate=0.4\n",
        "args1.aug=0\n",
        "\n",
        "for model_ in model_ls:\n",
        "  args_[(model_, 'amazon-photo')] = args1\n",
        "  args_[(model_, 'cora')] = args1\n",
        "  args_[(\"SAGE\", 'twitch-e')] = args1\n",
        "\n",
        "args1 = ARGS()\n",
        "args1.ent_filter=0.001\n",
        "args1.ep_tta=10\n",
        "args1.lr_tta=0.0001\n",
        "args1.train_all=True\n",
        "args1.not_reset=False\n",
        "args1.dropout_inference=4\n",
        "args1.dropout_rate=0.3\n",
        "args1.aug=0\n",
        "\n",
        "args_[(\"GCN\", 'elliptic')] = args1\n",
        "args_[(\"SAGE\", 'elliptic')] = args1\n",
        "\n",
        "args1 = ARGS()\n",
        "args1.ent_filter=0.001\n",
        "args1.ep_tta=10\n",
        "args1.lr_tta=0.0001\n",
        "args1.train_all=True\n",
        "args1.not_reset=False\n",
        "args1.dropout_inference=4\n",
        "args1.dropout_rate=0.3\n",
        "args1.aug=0\n",
        "\n",
        "args_[(\"GAT\", 'elliptic')] = args1\n",
        "\n",
        "\n",
        "args1 = ARGS()\n",
        "args1.ent_filter=0.2\n",
        "args1.ep_tta=30\n",
        "args1.lr_tta=0.0003\n",
        "args1.sam=False\n",
        "args1.train_all=True\n",
        "args1.not_reset=False\n",
        "args1.dropout_inference=4\n",
        "args1.dropout_rate=0.4\n",
        "args1.aug=0\n",
        "\n",
        "args_[(\"GCN\", 'twitch-e')] = args1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAyMET74vUB1",
        "outputId": "c5d108ab-5578-4d4f-c16b-66a7ca66723d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------- model GCN -------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "amazon-photo seed 9: 100%|| 10/10 [00:12<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amazon-photo [96.45 96.09 96.19 96.34 95.7  96.26 95.86 96.14 96.03 96.68]\n",
            "meanstd : 96.170.27\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cora seed 9: 100%|| 10/10 [00:06<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cora [98.2  98.27 98.73 98.57 98.21 98.13 98.41 98.33 98.31 97.94]\n",
            "meanstd : 98.310.21\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "elliptic seed 9: 100%|| 10/10 [01:51<00:00, 11.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elliptic [64.44 62.56 61.   60.89 61.   61.11 62.   62.33 63.33 63.33]\n",
            "meanstd : 62.201.17\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "twitch-e seed 9: 100%|| 10/10 [00:25<00:00,  2.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twitch-e [60.13 60.53 60.58 60.31 60.62 61.04 60.65 60.54 59.49 60.46]\n",
            "meanstd : 60.440.39\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for model_ in model_ls:\n",
        "  print(f\"------------- model {model_} -------------\")\n",
        "  for dataset_ in dataset_ls:\n",
        "    final_results = []\n",
        "    seed_bar = tqdm(seeds)\n",
        "    for seed_ in seed_bar:\n",
        "      seed_bar.set_description(f\"{dataset_} seed {seed_}\")\n",
        "      #######################################\n",
        "      args = args_[(model_, dataset_)]\n",
        "      # args = ARGS()\n",
        "      args.finetune = 1\n",
        "      args.tent = 1\n",
        "      args.use_learned_stats = 0\n",
        "      args.bn_momentum=0.0\n",
        "\n",
        "      # args.ent_filter=0.3\n",
        "      # args.ep_tta=10\n",
        "      # args.lr_tta=0.00001\n",
        "      # args.sam=False\n",
        "      # args.train_all=True\n",
        "      # args.not_reset=False\n",
        "      # args.dropout_inference=4\n",
        "      # args.dropout_rate=0.4\n",
        "      # args.aug=0\n",
        "\n",
        "      args.verbose=False\n",
        "\n",
        "      args.model = model_\n",
        "      args.dataset = dataset_\n",
        "      args.seed = seed_\n",
        "      #######################################\n",
        "\n",
        "      lr_feat = args.lr_feat; epochs = args.epochs; ratio = args.ratio; lr_adj = args.lr_adj\n",
        "      # print('===========')\n",
        "      reset_args(args)\n",
        "      if args.model == 'GAT':\n",
        "          args.loop_adj = 0; args.loop_feat = args.epochs\n",
        "      if args.tune: # set args.tune to 1 to change the model hyperparameters\n",
        "          args.lr_feat = lr_feat; args.epochs = epochs; args.ratio = ratio; args.lr_adj = lr_adj\n",
        "      if args.epochs == 2:\n",
        "          args.loop_adj = 1; args.loop_feat = 1\n",
        "\n",
        "      # print(args)\n",
        "\n",
        "      # from utils import get_gpu_memory_map\n",
        "      mem_st = get_gpu_memory_map()\n",
        "\n",
        "\n",
        "      if args.dataset == \"cora\":\n",
        "        data = [dataset_tr_cora, dataset_val_cora, datasets_te_cora]\n",
        "      elif args.dataset == \"amazon-photo\":\n",
        "        data = [dataset_tr_amazon_photo, dataset_val_amazon_photo, datasets_te_amazon_photo]\n",
        "      elif args.dataset == \"elliptic\":\n",
        "        data = [dataset_tr_elliptic, dataset_val_elliptic, datasets_te_elliptic]\n",
        "      elif args.dataset == \"fb100\":\n",
        "        data = [dataset_tr_fb100, dataset_val_fb100, datasets_te_fb100]\n",
        "      elif args.dataset == \"twitch-e\":\n",
        "        data = [dataset_tr_twitch_e, dataset_val_twitch_e, datasets_te_twitch_e]\n",
        "      else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "      # random seed setting\n",
        "      random.seed(args.seed)\n",
        "      np.random.seed(args.seed)\n",
        "      torch.manual_seed(args.seed)\n",
        "      torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "      res = []\n",
        "      agent = GraphAgent(data, args)\n",
        "\n",
        "      if args.dataset != 'elliptic':\n",
        "          y_te, out_te = [], []\n",
        "          for ix, test_data in enumerate(data[-1]):\n",
        "              if args.finetune:\n",
        "                  acc, output, labels = agent.finetune(test_data)\n",
        "              else:\n",
        "                  acc, output, labels = agent.learn_graph(test_data)\n",
        "              res.append(acc)\n",
        "              y_te.append(labels)\n",
        "              out_te.append(output)\n",
        "\n",
        "              if args.debug == 2:\n",
        "                  break\n",
        "          acc_te = agent.model.eval_func(torch.cat(y_te, dim=0), torch.cat(out_te, dim=0))\n",
        "\n",
        "      else:\n",
        "          y_te_all, out_te_all = [], []\n",
        "          y_te, out_te = [], []\n",
        "          for ii, test_data in enumerate(data[-1]):\n",
        "              if args.finetune:\n",
        "                  acc, output, labels = agent.finetune(test_data)\n",
        "              else:\n",
        "                  acc, output, labels = agent.learn_graph(test_data)\n",
        "              y_te.append(labels)\n",
        "              out_te.append(output)\n",
        "\n",
        "              y_te_all.append(labels)\n",
        "              out_te_all.append(output)\n",
        "\n",
        "              if ii % 4 == 0 or ii == len(data[-1]) - 1:\n",
        "                  acc_te = agent.model.eval_func(torch.cat(y_te, dim=0), torch.cat(out_te, dim=0))\n",
        "                  res += [float(f'{acc_te:.2f}')]\n",
        "                  y_te, out_te = [], []\n",
        "                  if args.debug==2:\n",
        "                      break\n",
        "\n",
        "          acc_te = agent.model.eval_func(torch.cat(y_te_all, dim=0), torch.cat(out_te_all, dim=0))\n",
        "\n",
        "          # print('Results on test sets:', res)\n",
        "          # print(f'Mean result on {args.dataset}:', np.mean(res))\n",
        "\n",
        "      final_results += [np.mean(res)]\n",
        "    final_results = np.array(final_results) * 100.0\n",
        "    print(f\"{args.dataset}\", np.round(final_results,2))\n",
        "    print(f\"meanstd : {final_results.mean():.2f}{final_results.std():.2f}\")\n",
        "    print('')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wokSiZ7aeuc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ma0jNXMnHIQ8",
        "-5uzGMXIN6hx",
        "vJldxM2YN97F",
        "CMHeFfALOIVe",
        "PoqsZ5h6OOZa",
        "CYykYQUqPdvi",
        "yy5P7EycP3y5",
        "oNTF2mSyPuFY",
        "uLeFglToP-jO",
        "tbKN74d_TC1q",
        "Wm-LsexlTMZj",
        "HYkdDZwHTRtp",
        "7eFFwPb4gjY-",
        "tJKk2Zw_ucit",
        "3S6XH80UuSDo",
        "PQbtKiqvIBwV",
        "B_nE-IkmIG2J",
        "EFV3-GhH0b8R",
        "VSO29z_L0fM2",
        "v-tkRBPa0ifw",
        "NxzUVxrgKzl-"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}